#+TITLE: *Modelling the effects of domestication in Wheat through novel computer vision techniques*
#+OPTIONS: title:nil toc:nil H:4 author:nil date:nil TeX:t LaTeX:t  ^:nil
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+INCLUDE: "./preamble.org"

* Introduction, Analysis and Objectives

This project aims to answer a biological research question through the use of computer science, whilst also creating a software suite which will enable further studies to be carried out with ease.

Primarily the focus has been on the data science elements of my degree, creating, cleaning and discerning meaning in it.

Using a population of genetically diverse wheat, several hypothesis and questions are explored in the hopes of contributing to the scientific understanding of plant domestication. A mixture of image analysis through three-dimensional micro-computed tomography and computational analysis are used to provide these much needed solutions.


Additionally, as this is very much multi-disciplinary research, specific terms and definitions have been outlined in the glossary (table:[[tab:glossary]]).

** Background

   Western society and agriculture has been dominated by the ability to create successful crops for the past 10,000 years cite:Ozkan2002. Of these crops wheat is considered to be one of the most vital and is estimated to contribute to 20% of the total calories and proteins consumed worldwide, and accounts for roughly 53% of total harvested area (in China and Central Asia) cite:Shiferaw2013.

During domestication, the main traits selected for breeding were most likely plant height and yield. This meant that important non-expressed traits such as disease resistance and drought tolerance were often neglected and lost overtime.

Whilst the choices made for selective breeding were successful, effects are now being felt as it is estimated that as much as a 5% dip is observed yearly for  wheat production cite:Shiferaw2013. This decrease in efficiency is attributed to climate change bringing in more hostile conditions, which these elite and  domesticated genotypes are unprepared for.

Furthermore, with increasing population and less arable land there is an even greater pressure for the optimisation of grain and spike characteristics. With studies showing that spikelet, the collective for seeds sharing the same node on a spike, count can be controlled by specific and sometimes recessive genes cite:Finnegan2018, which could drastically enhance overall yield, and a general public distrust towards genetically modification cite:Aleksejeva2014,Twardowski2015,Lynas the reliance on breeding programs for optimisation is further stressed.

Modern breeding programs have had some success in selecting primitive undomesticated genotypes and using them to breed back in useful alleles which would have been lost during domestication cite:Charmet2011.

As such, there are questions still left open about how best to make selections for crop breeding. There is also a lack of formalised modelling of information which could be of use in these areas of research.

** Biological Question and Materials

The driving question for this research asks "Can \micro-Computed Tomography data be used to model domestication in wheat?". Using an already grown and harvested range of genetically diverse wheat this project has generated a collection of 3D images, processed these images into raw phenotypic data and produced biologically significant information.

Here, ploidy has been used as a distinguishable and important attribute of the study. Ploidy refers to the number of sets of chromosomes that an organism has, usually this is denoted by an N.

The genotypes used in this study are listed here, denoted by "/X/ N" where /X/ indicates the ploidy. 2N - Diploid; 4N - Tetraploid; 6N - Hexaploid. A /W/ indicates wild genotypes, /D/ domesticated ones.

#+BEGIN_EXPORT latex
\begin{multicols}{3}

  \begin{itemize}
  \item{\textit{T. boeoticum} (2N|W)}
  \item{\textit{T. monococcum} (2N|D)}
  \item{\textit{A. Tauschii} (2N|D)}
  \end{itemize}

  \columnbreak

  \begin{itemize}
  \item{\textit{T. durum} (4N|D)}
  \item{\textit{T. dicoccum} (4N|D)}
  \item{\textit{T. dicoccoides} (4N|W)}
  \item{\textit{T. ispahanicum}(4N|D)}
  \item{\textit{T. timopheevii} (4N|D)}
  \end{itemize}

  \columnbreak

  \begin{itemize}
  \item{\textit{T. spelta} (6N|D)}
  \item{\textit{T. aestivum} (6N|D)}
  \item{\textit{T. compactum} (6N|D)}
  \end{itemize}

\end{multicols}
#+END_EXPORT
Full species names are found in table:[[tab:wheat]].

*** Why use \micro-CT image analysis?
In the past, science has been greatly limited by the amount of data which could be processed in an experiment. In the last few decades the inclusion of computer science has reduced this bottleneck. Now, the challenge for many fields of research is producing more data and this is often cited as the new limiting factor in creating robust studies cite:Furbank2011.

Many experiments aim to meet the demand for data by using high-throughput automated imaging systems cite:Naumann2007,Prasanna2013,Humplik2015. These systems have, in the last decade, become a standard and accepted tool for data generation in plant phenotype studies. However, they will only produce 2-dimensional data on a per-plant basis. Image processing research has had success in modifying these automated systems in order to produce a pseudo 3-dimensional structure using stero-imaging cite:Roussel2016. Even so, these techniques require destructive harvesting of plant materials and do not provide information of internal structure.

For decades medical research has found success with X-Ray imaging technology cite:Wang2008. From this, plant science has been able to benefit from the wealth of prior knowledge and more and more studies are being augmented with the use of X-Ray/\micro-CT imaging cite:Jhala2015,Tracy2012,Metzner2015,Hughes2017,Staedler2013.

In this study, \micro-CT has enabled the study of individual seeds of wheat, which is the product that plant breeders, commercial growers and farmers are truly interested in. Other imaging techniques could not provide as much detail, whilst retaining developmental and positional information, or in such a high throughput or quality/resolution.

*** Extracted Data

 These samples come from over 70 plants and provided in excess of 2000 seeds. These data form the basis for this research and testing of the software libraries created. The traits recorded are labelled in figure:[[fig:seeds]] and are as follows:

 #+BEGIN_EXPORT latex
   \begin{multicols}{2}

     \begin{itemize}
     \item Length
     \item Width
     \item Depth

     \end{itemize}

     \columnbreak

     \begin{itemize}
     \item Volume
     \item Surface Area
     \item Crease Depth / Volume
     \end{itemize}
   \end{multicols}
 #+END_EXPORT


 #+CAPTION: Wheat grain labelled (/left/), wheat grain cut in half horizontally (/right/), adapted from Hughes et al. cite:Hughes2017
 #+ATTR_LATEX: :width 17cm
 #+NAME: fig:seeds
 [[./images/seeds.png]]

** Significance to Current Research
The biological interest in this area has been expressed in several areas of research cite:Leigh2013, it is proposed that the key to unlocking diversity in the wheat genus lies in these ancestor, undomesticated species cite:Cockram2007.

This research has the potential to be useful in several areas including: crop breeding; disease resistance; environmental stress. Each of these areas depend on making informed decisions in order to direct experiments. By producing information at an individual seed level, this study has been able to provide data that can offer suggestions of grain potential and the developmental behaviour/effects of polyloidization.

Often, the most sought after traits are centred around thousand-grain-weight (TGW) as well as standard deviation of seed shapes. During harvesting, filters are used to only allow ideal shaped seeds through. This means that, potentially, despite a breed of wheat providing a high average volume of seed in reality much of it may go to waste if the shapes are not uniform. This research aims to alleviate this problem and provides low level information which is sorely required.

The individual images in figure:[[fig:phylo]] show, at a glance, the diversity and also the difference in the wild and cultivated (domesticated)
species. This work allows for these differences to be quantified and evaluated into useful metrics for answering research based questions.

By better understanding the morphometric deviations in wheat species, more informed choices can be made when it comes to breeding wheat for the future and to fulfil ever-changing requirements.

\clearpage

#+CAPTION: Phylogeny of wheat genotypes (Provided by Dr. Hugo Oliveira)
#+ATTR_LATEX: :width 17cm
#+NAME: fig:phylo
[[file:./images/philotree.png]]

** Aims and Objectives

The overarching aim of this project has been to create several pieces of software which aid in answering the biologically significant questions outlined. As well as to prove/disprove the hypothesis stated below.

The software created is robust in order to duplicate results and is flexible as to allow for further studies to be carried out and to use the same method.

Novel additions have been made to existing image analysis libraries in order to make them more flexible for this project. Figure:[[fig:spikes]] illustrates the range of diversity

Furthermore, the library written allows for easy data organisation and automation of otherwise difficult tasks such as concatenating data from multiple sources and graphing of information. Full documentation and integrated testing allows for a suite of tools which can be built upon in future and reduce the amount of effort required for similar studies to be carried out and analysed.

** Hypothesis
To provide a full spectrum of analysis the null-hypothesis of this work is presented as investigating if there are morphometric differences in the seeds of several wheat varieties outlined in figure:[[fig:phylo]].

The comparison pairs are as follows (where W indicates wild genotypes and D domesticated.):

1. /T. monococcum/ (2N|D) and /T. beoticum/ (2N|W)
2. /T. dicoccum/ (4N|D) and /T. dicoccoides/ (4N|W)
3. /T. spelta/ (6N|D) and /T. aestivum/ (6N|D)
4. /T. dicoccum/ (4N|D) and /T. durum/ (4N|D)
5. /T. beoticum/ (2N|W) and /T. dicoccoides/ (4N|W)
6. /T. dicoccum/ (4N|D) and /T. aestivum (6N|D)
7. /T. durum/ (4N|D) and /T. aestivum (6N|D)

These comparison groups where chosen, with help from researchers at the National Plant Phenomics Centre and the University of Manchester, based on their ploidy and also on their domestication grouping. This maximises the potential of this research by isolating features and attributes of wheat based on domestication status.
#+CAPTION: Scans of wheat, showing diversity in Population, Compactum (6N) left, Durum right (4N)
#+ATTR_LATEX: :width 10cm
#+NAME: fig:spikes
[[./images/spikes.png]]

** Challenges Overview

The challenges which this project tackles come in two flavours: Computational and Biological. As such keen awareness of these is needed to appreciate the novelty of this work.

*** Biological Challenges
Previous studies have been able to demonstrate that variation in wheat grain morphology can be partially explained, in 2010 Gegas et al. demonstrated this through a 99.4% 2 component PCA cite:Gegas2010. However there is much left to do in terms of formal classifications and descriptions of these differences. This project deals with this problem through computational analysis.

Two effects run parallel in this study, both of these need to be accounted for in the analysis, and questions need asked in a manner to extract each effect independently:

1. The effects of polyploidization in wheat.
2. The effects of domestication in wheat.

Both of these effects will need to be taken into consideration so as not to misidentify results, and falsely attribute effect.

*** Computational Challenges
Using \micro-CT data in plant sciences is becoming more and more common cite:Tracy2017,Jhala2015,Hughes2017,Metzner2015 and whilst a lot of studies focus on the traits of grains specifically no formal model has been created, and no accepted data format. This is a data engineering problem and the methods described in this project address this.

Further to data organisation, proposals are made for the statistical analysis which should be used. This allows for studies to become more robust and repeatable, thus strengthening the studies overall and giving more confidence to results.

The biological material used in this research is much more diverse a population than has been previously studied with \micro-CT image analysis. This requires current computer vision methods to be adapted in order to analyse this added diversity.

** Deliverables

This project provides three final deliveribles:

1. A flexible software suite written in /Python/ that provides a standardised method for analysing and interpreting \micro-CT data output.
2. A Graphical User Interface (GUI) which offers a point and click method for data gathering, graphing and manipulating \micro-CT data, using the library from deliverable 1 as a backend.
3. Answers to the proposed questions (hypotheses), provided in the /Results/ and /Discussion/ sections of this report.

* Software Design, Implementation and Testing
This chapter outlines choices and methodologies employed in the software engineering aspect of this project, as well as highlighting the key functional requirements and implementation decisions.

** Functional Requirements
Requirements for this project are split between software requirements for both the CT Analysing Library and the CT GUI Application and the research requirements (i.e. the answers to the proposed hypothesis). Here the requirements for the software are discussed:
*** Requirements for CT Analysing Library

These are the functional requirements for the Python library produced:

#+BEGIN_EXPORT latex
\begin{multicols}{2}

  \begin{enumerate}
  \setcounter{enumi}{-1}
  \item Provide an OOP means to deal with data
  \item Make gathering of data simplified
  \item Handle Saving of data in a usable format
  \item Easily enable data transformations
  \item Perform hypothesis testing
  \end{enumerate}

  \columnbreak

  \begin{enumerate}
  \setcounter{enumi}{4}
  \item Process rejoining of split scans
  \item Handle Removing of erroneous data
  \item Enable matching data to external information
  \item Auto plot data (boxplots, histograms etc.)
  \item Allow easy filtering of data
  \end{enumerate}

  \columnbreak

\end{multicols}
#+END_EXPORT

*** Requirements for CT GUI Application


#+BEGIN_EXPORT latex
\begin{multicols}{2}

  \begin{enumerate}
  \setcounter{enumi}{9}
  \item Provide a intuitive user interface for working with CT data
  \item Allow a interaction with data without the need for programming
  \item Implement the Matplotlib plotting utility
  \item Easily join experiment data with CT data
  \item Use an MVC model
  \end{enumerate}

  \columnbreak

  \begin{enumerate}
  \setcounter{enumi}{14}
  \item Implement the CT Analysis Library
  \item Display data visually
  \item Dynamically create graphs
  \item Provide hypothesis testing
  \end{enumerate}

  \columnbreak

\end{multicols}
#+END_EXPORT

** Software Development Methodology
This project made use of formal design methods and strict organisation whilst being flexible to change. Overall the design took a hybridised form in order to best suit the scientific environment which this domain specific software is built for.

Data analysis drove the direction of the project, as a result an agile methodology was adopted.
Weekly sprints were implemented as a list of "todo's", these were written on a Monday morning based on the previous week's list.

Critical self-evaluation was performed by means of a "one-man SCRUM" meeting, this is a technique which requires self-discipline in order to accurately find faults and areas for improvement cite:Andrews.

Further to this, regular meetings with research staff, at the National Plant Phenomics Centre,  allowed for a developer-client relationship which SCRUM defines as being key. During these meetings details of the research was discussed and ideas given as to how future experiments could proceed. This allowed for critical decisions to be made regarding software design and overall structure.
** Sprint Timeline
The implementation of this work was done following agile sprint planning, treating each week as a encapsulated working frame, for each of these a detailed organisational programme was created, discussed with supervisors and then used to formulate plans of action for following up on.

A full account of the sprints, which are in part running documentation for this project, are available in appendix:[[sec:appendix]].

** Language Choices
Both the CT Analysing Library and the CT Analysing GUI are implemented using the Python programming language, it has been developed and tested in versions 3.5 and 3.6 (Python 2 is not supported at all by this project, there is an ongoing effort to move forward in scientific computing, away from Python 2.X  cite:Ozgur2016).

In scientific programming three of the most commonly used languages are Python, R and MATLAB cite:Ozgur2016.

These three languages are able to provide all the features which this project requires. However Python was chosen for several reasons.

MATLAB could not be used as a potential language due to it being pay to use software, as this project aims to be accessible, the cost of software would greatly reduce the scope of access.

R is a valid candidate, it provides all of the statistical capabilities required by the project, it also provides packages for creating GUI based applications, it is fast and it is widely used in scientific computing and data science.

The main deciding factor is Python's wealth of resources, adoption rate and the developer of this project being vastly more experienced with Python's ecosystem than R's.

** Designing Process
Through meetings and emails, the agile principles of communication over comprehensive documentation was used. Where conversations were decidedly much more beneficial than complex planing prior to developing a product.

Graphical elements, such as the graphing functionality of the CT Analysing Library and the CT GUI Application were sketched using wire-frames whilst in meetings where the potential users (clients) could provide their ideas.

In figure:[[fig:gui1]] an example of the wire-frames created during meetings is show (A), next to it is displayed the final look of the loading window (B).
 #+CAPTION: Wire-frame (A) of the GUI loading data window (B)
 #+ATTR_LATEX: :width 16cm
 #+NAME: fig:gui1
 [[./images/wireframe1.png]]

Similarly, figure:[[fig:gui2]] provides the initial wire-frame (A) of how the analysis window could have looked and what kind of GUI elements would be required, again, next to it is the final analysis window (B)
 #+CAPTION: Wire-frame (A) of the GUI analysis window (B)
 #+ATTR_LATEX: :width 16cm
 #+NAME: fig:gui2
 [[./images/wireframe2.png]]
** Documentation
Whilst an agile approach was used, some documentation was created for use with the CT Analysing Library.

The provided CT Analysing Library comes with "human-readable" format. Where as most documentation generators (Doxygen, Pydocs, Javadocs etc.) implement very well structured and comprehensive documentation, the output is generally not very friendly and easy to read. Particularly for non-career-programmers. A core feature of these provided software implementations are that they are well suited for a biologist, researcher or statistician to use.

This documentation generator was purpose created, implemented in LISP and provided in listing:[[lst:docgen]].

Beyond this, inline commenting is provided for supplied software. Keeping in line with the agile development ethos the software is self-documented and self-evident. A brief example of this is shown in listing:[[lst:docexample]]

Documentation for the CT GUI application is provided as a visual user guide, and provides sample data for the user to test with.
** Software Library Choices
The software libraries used for this project focus around data manipulation. Where possible, only core libraries of the Python language were used and only well supported, established and documented libraries were chosen. Software support is a major requirement for reproducible results.

All software packages used in the Analysing Library are required by the CT Analysing GUI as the Library is a dependency of it. The GUI has a single separate requirement /PyQT5/.
Table:[[tab:software]] contains a full listing of all software used and required by this project.
*** Numpy
The Numpy library is one of the most commonly used additions to the Python ecosystem, it is fundamental to many data science projects. Here it is used to handle data lists, arrays and structures. The data model used in this project is able to be optimised through Numpy and allows results to be calculated in seconds.
*** Matplotlib/Seaborn
Matplotlib acts as the plotting backend for the project. The Seaborn package acts as a porcelain for matplotlib and makes graph creation and decoration much easier.
*** SciPy
Data transforms such as Box Cox and PCA are dependant on the functions of the Scipy library. Alternatives are available, however this is the most well established and often used library for these functions. SciPy provides many of the required functions in a single package and helps reduce further software dependencies.
*** Pandas
Pandas is used to read the CSV files which the raw data is stored in. This library converts and stores data in dataframes which are used throughout this project to manipulate data. Dataframes are used as the core of /CTData/ objects in the analysis library.
*** Xlrd
This extension library is required in order to read Microsoft encoded files. Extra experiment information can be provided with the "xlsx" extension.
*** Statsmodels
The backbone of the data analysis model is handled by this library, it also provides an implementation of the Markov chain Monte Carlo system used to generate random samples from the model population.
*** PyQT5
There were many options for creating a user interface in Python, the language provides its own core library via the /TKinter/ module. However PyQT is a port of the QT framework, one of the most widely used libraries for GUIs in software development. It is cross platform, robust and has excellent documentation and user-guides.

** Version control
This project has used Git version 2.7.4 throughout. The structure of the project has been as submodules of a larger project.

By using submodules the CT Grain Analysing Library could be kept in sync with the GUI aspect of the project.

Additionally, /setup.py/ has been used to provide installation of the library, the code for this can be seen in listing:[[lst:setuppy]]. Using /setup.py/ provides a quick and easy way for any user to install the software, along with any dependencies.

**** Issue Tracking
Issues were tracked during the project, both in personal notes and in the Git interface as illustrated in figure:[[fig:github]]
#+CAPTION: Github Issue Tracking
#+NAME: fig:github
#+ATTR_LATEX: :width 10cm
[[./images/github.png]]

** Implementation Methodology
Strict software engineering principles were applied during creation of this project. The use of standards, design patterns and code-linters have been used throughout to minimise the possibility of errors and to create wholly extendable software. These devices enable understandable and self-documented code allowing future users to quickly start using the provided packages.
*** Standards
The main standard adhered to for software provided by this project is the PEP8 style guide cite:VanRossum. The principle behind this coding style, as stated by Guido van Rossum, is "Code is read much more often than it is written". This makes this styling guide perfect for the chosen agile methodology of self-evident documentation in the software.

In addition to PEP8, a Python code linter Flake8 has been used to prevent "code smells", bad formatting, incorrect white space usage etc.

*** CT Analysing Library Design Pattern
The CT Analysing Library uses a Singleton style design pattern. A single data object is created from a /CTData/ class.

A very functional paradigm is used by this library. By applying mapping and filter style functions data elements can be passed to the supporting modules: /data_transforms.py/; /graphing.py/, /statistical_tests.py/. These modules enable scientific functions to be applied to the /CTData/ object. A UML style class diagram is shown in figure:[[fig:ctdata]], here the interactions of the classes can be seen, as well as their internal functions.

#+NAME: fig:ctdata
#+CAPTION: CT Analysing Library UML
#+ATTR_LATEX: :width 12cm
[[file:./images/ctdata.png]]

*** CT GUI Application Design Pattern
The Model-View-Controller (MVC) design pattern is one of the most commonly structures for creating user interfaces. It allows for the user's view/interface code to be separated from the model, the code which changes the data. The model and the view communicate and update each other via the controller element of the design.

The QT framework provides "connectors" which act as triggers/activations for functions, these are set off by the user providing either keyboard or mouse based input.

#+NAME: fig:ctgui
#+CAPTION: CT Analysing GUI UML
#+ATTR_LATEX: :width 15.2cm
[[file:./images/ctgui.png]]
** Library Versioning
The development of this project was performed using the Python /virtualenv/. This is a virtual environment package which Python offers, it allows for an isolated working copy of the project.

By developing in this manner, libraries were ensured to be using the correct versions required by the software.
 \clearpage
** Testing
Testing was performed both in acceptance testing by using user feedback, the functional requirements and the ability to use the software to answer the hypothesis of the research elements of this project. Further to this, unit testing was performed to allow for automated testing as well as test-driven-development of features.
*** Feedback Forms
Feedback and constructive suggestions were made by researchers at the National Plant Phenomics Centre, these were submitted via the Google forms service.

These provided a method of acceptance testing by those who would be using the software to help with investigating data. Page 1 of the given form is shown in figure:[[fig:feedback]]. This form was completed by 3 researchers at the National Plant Phenomics Centre, feedback was very positive overall.
#+NAME: fig:feedback
#+CAPTION: CT Feedback form
#+ATTR_LATEX: :width 7.3cm
[[./images/feedbackform.png]]

\clearpage

*** Unit Testing CT Analysing Library
The unit tests for the CT Analysing Library were straightforward, using the /PyTest/ framework and a subset of data from a data set, these tests assert that features are implemented correctly and that the correct results are given.


#+ATTR_LATEX: :environment tabularx :width \textwidth :align |l|l|X|
#+NAME: tab:unittest
#+CAPTION: Output of /pytest/ Unit Tests and results for CT Analysing Library
|--------+----------+----------------------------------------------------|
| *I.D.* | *Result* | *Test*                                             |
|--------+----------+----------------------------------------------------|
|      0 | Passed   | CTData.py::test_aggregate_spike_averages           |
|--------+----------+----------------------------------------------------|
|      1 | Passed   | CTData.py::test_clean_data_maximum_removed         |
|--------+----------+----------------------------------------------------|
|      2 | Passed   | CTData.py::test_clean_data_minimum_removed         |
|--------+----------+----------------------------------------------------|
|      3 | Passed   | CTData.py::test_load_additional_data               |
|--------+----------+----------------------------------------------------|
|      4 | Passed   | CTData.py::test_load_additional_data_no_data       |
|--------+----------+----------------------------------------------------|
|      5 | Passed   | CTData.py::test_load_data                          |
|--------+----------+----------------------------------------------------|
|      6 | Passed   | CTData.py::test_NoDataFoundException               |
|--------+----------+----------------------------------------------------|
|      7 | Passed   | Data_transforms.py::test_box_cox_data              |
|--------+----------+----------------------------------------------------|
|      8 | Passed   | Data_transforms.py::test_pca_to_table              |
|--------+----------+----------------------------------------------------|
|      9 | Passed   | Data_transforms.py::test_perform_pca               |
|--------+----------+----------------------------------------------------|
|     10 | Passed   | Data_transforms.py::test_standardise_data          |
|--------+----------+----------------------------------------------------|
|     11 | Passed   | Graphing.py::test_plot_boxplot_as_dataframe        |
|--------+----------+----------------------------------------------------|
|     12 | Passed   | Graphing.py::test_plot_boxplot_as_object           |
|--------+----------+----------------------------------------------------|
|     13 | Passed   | Graphing.py::test_plot_difference_of_means         |
|--------+----------+----------------------------------------------------|
|     14 | Passed   | Graphing.py::test_plot_histogram_as_dataframe      |
|--------+----------+----------------------------------------------------|
|     15 | Passed   | Graphing.py::test_plot_histogram_as_object         |
|--------+----------+----------------------------------------------------|
|     16 | Passed   | Graphing.py::test_plot_pca                         |
|--------+----------+----------------------------------------------------|
|     17 | Passed   | Graphing.py::test_plot_qqplot                      |
|--------+----------+----------------------------------------------------|
|     18 | Passed   | Statistical_tests.py::test_baysian_hypothesis_test |
|--------+----------+----------------------------------------------------|
|     19 | Passed   | Statistical_tests.py::test_t_test                  |
|--------+----------+----------------------------------------------------|
|     20 | Passed   | Statistical_tests.py::test_test_normality          |
|--------+----------+----------------------------------------------------|


\clearpage

*** Unit Testing CT GUI Application

The unit testing used for the CT GUI Application was more sophisticated than that of the Library. This testing required visual confirmation that figures and graphs generated were displayed correctly and that they showed what the user would expect, given the data.

To do this a /PyTest/ plugin was used, /QtBot/ which provides simulated user input. This allows for the GUI to be thoroughly tested, automatically.

In table:[[tab:unittest2]] the results of the automated testing is given along side an image of several of the tests, tests of the same graphs but with different parameters were also generated and manually verified and provided as supplemental data.

#+ATTR_LATEX: :environment longtable :width \textwidth :align |l|l|p{4.4cm}|C|
#+NAME: tab:unittest2
#+CAPTION: Output of /pytest/ Unit Tests and results for CT GUI Application
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
| *I.D.* | *Result* | *Test*                                                 | *Image*                                                      |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     21 | Passed   | analysis.py:: box_groupby_1_rb_1                       | [[./images/Screenshots/analysis_window_box_groupby_1_rb_1.png]]  |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     22 | Passed   | analysis.py:: box_groupby_2_rb_2                       | [[./images/Screenshots/analysis_window_box_groupby_2_rb_2.png]]  |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     23 | Passed   | analysis.py:: box_rb_1                                 | [[./images/Screenshots/analysis_window_box_rb_1.png]]            |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     24 | Passed   | analysis.py:: hist_groupby_1_rb_1                      | [[./images/Screenshots/analysis_window_hist_groupby_1_rb_1.png]] |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     25 | Passed   | analysis.py:: hist_rb_1                                | [[./images/Screenshots/analysis_window_hist_rb_1.png]]           |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     26 | Passed   | hypothesis_tests.py:: bayesg1_att_1                    | [[./images/Screenshots/hypothesis_bayestest_g1_att_1.png]]       |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     27 | Passed   | hypothesis_tests.py:: tg1_att_1                        | [[./images/Screenshots/hypothesis_ttest_g1_att_1.png]]           |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     28 | Passed   | analysis.py:: box_rb_2                                 | N/A                                                          |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     29 | Passed   | analysis.py:: hist_groupby_1_rb_2                      | N/A                                                          |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     30 | Passed   | analysis.py:: hist_rb_2                                | N/A                                                          |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     31 | Passed   | analysis.py:: loads                                    | N/A                                                          |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     32 | Passed   | hypothesis_tests.py:: bayesg1_att_2                    | N/A                                                          |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     33 | Passed   | hypothesis_tests.py:: bayesg2_att_1                    | N/A                                                          |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     34 | Passed   | hypothesis_tests.py:: bayesg2_att_2                    | N/A                                                          |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     35 | Passed   | hypothesis_tests.py:: tg1_att_2                        | N/A                                                          |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     36 | Passed   | hypothesis_tests.py:: tg2_att_1                        | N/A                                                          |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     37 | Passed   | hypothesis_tests.py:: tg2_att_2                        | N/A                                                          |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     38 | Passed   | hypothesis_tests.py:: loads                            | N/A                                                          |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     39 | Passed   | GUI.py:: startup                                       | N/A                                                          |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     40 | Passed   | load_data.py:: load_data_with_rachis                   | N/A                                                          |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     41 | Passed   | load_data.py:: load_data_without_rachis                | N/A                                                          |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     42 | Passed   | preprocessing.py:: clean_data_remove_large             | N/A                                                          |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     43 | Passed   | preprocessing.py:: clean_data_remove_none              | N/A                                                          |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     44 | Passed   | preprocessing.py:: clean_data_remove_small             | N/A                                                          |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     45 | Passed   | preprocessing.py:: clean_data_remove _small_and_large  | N/A                                                          |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     46 | Passed   | preprocessing.py:: load_additional_data                | N/A                                                          |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|
|     47 | Passed   | preprocessing.py:: load_additional_data _expected_fail | N/A                                                          |
|--------+----------+--------------------------------------------------------+--------------------------------------------------------------|

*** Acceptance Testing
These unit tests, alongside user tests have been used to meet the outlined functional requirements; specific tests are shown in table:[[tab:frtests]] to match them to the associated functional requirement.

#+ATTR_LATEX: :environment longtable :width \textwidth :align |l|l|p{14cm}|
#+NAME: tab:frtests
#+CAPTION: Functional requirements and Unit tests
|--------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| *F.R.* | *U.T.*         | *How is the F.R. met?*                                                                                                                                                                           |
|--------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|      0 | n/a            | An OOP interface is provided by the /ct_data/ object                                                                                                                                             |
|--------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|      1 | 3,4,5          | Loading of data is tested multiple times specifically through unit tests                                                                                                                         |
|--------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|      2 | 4,5            | Unit tests carry out saving loaded data into appropriate formats, in particular python /pandas/ tables and CSV files, this was also user tested                                                  |
|--------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|      3 | 7,8,9,10       | All the functions in the /data_transforms.py/ library are unit tested automatically on test data of known output                                                                                 |
|--------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|      4 | 26,27,32-38    | Hypothesis testing was tested with known data 8+ times in automated unit testing for both CT GUI Application and the CT Analysis Library group of tests , testers also used these functions      |
|--------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|      5 | 6,7            | Loading of spikes was tested both for data which was known to exist and could be successfully done, and for data which was know to throw errors                                                  |
|--------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|      6 | 1,2,7,42-46    | Finding and removing erroneous data was unit tested by having a combination of parameters tested for both the GUI and the Analysis Library                                                       |
|--------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|      7 | 3-6,40,41      | Matching of pre-existing experiment information to the extracted data was tested for bad inputs through unit testing, users tested this too and reported no unexpected results                   |
|--------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|      8 | 11-17,28-38    | Plotting of data automatically, inferring of axis, titles and measurements was tested, with images automatically recorded by unit tests.                                                         |
|--------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|      9 | 0,1,2,3        | Unit testing checked if given data could easily be separated and divided into subsections, this was specifically tested and tested by proxy in other tests when hypothesis testing was performed |
|--------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|     10 | 21,26 39,40,42 | Multiple tests are taken to check that the GUI loads as expected, planning sessions and user feedback was also used to meet this particular functional requirement                               |
|--------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|     11 | 40,42          | Data is loaded through GUI elements with little prior knowledge of the software in order to have data processed, this was unit tested and user feedback was used to make more accessible         |
|--------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|     12 | 21-27          | Testing of /Matplotlib/ and /Seaborn/ interfacing with, and integrating with the CT Analysing GUI.                                                                                               |
|--------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|     13 | 43-47          | Loading of experiment data is pivotal for most functionality of the GUI, it is tested multiple times in an isolated formatted, as well as by other future functions which require it             |
|--------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|     14 | n/a            | The entire software is wrapped in an MVC model, this cannot be tested but the functional requirement has been met                                                                                |
|--------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|     15 | 21-47          | All of the unit tests for the CT Analysing GUI depend on the CT Analysing Library, it is reasonable that the integration is well tested, by product of all other tests                           |
|--------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|     16 | 21-27          | Several tests, those shown in table:[[tab:unittest2]] are specifically recorded visually and presented as images in order to automatically unit test that visual data is intact                      |
|--------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|     17 | 21-27          | By using automatic image capturing of unit testing in progress, visual checking can be carried out and confirmed that data is visually what is expected and to a high standard                   |
|--------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|     18 | 26,27, 32-28   | With multiple known outputs, several different groupings and combinations the hypothesis tests were unit tested and this functional requirement is known to be acceptably implemented            |
|--------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|

* Methods and Solutions
This chapter will discuss and expand upon the software implementations which were carried out in the course of this project, with particular focus on novel and non-trivial elements of software engineering.

Where appropriate code segments are provided to illustrate how a feature has been implemented in software.

** Data Processing Methods
This project presents a start to finish style of data analysis, data are generated by extracting information from raw scans, the resulting data is then post-processed and statistical analysis draws information from this.

Historically, these processes and tasks are huge time sinks in terms of manual work, this project emphasises an increase in automation.

Two separate pipelines are provided to carry out these tasks.

*** Image-Data Requisition Pipeline

Acquiring data for this study first required Wheat plants to be grown and harvested, the process outlined here begins with scanning samples from these plants in a \micro-CT machine.

These scans are processed by MATLAB cite:MATHWORKS2017. This process is defined as a method by Hughes et al. cite:Hughes2017, new and novel additions are added in the watershedding and segmentation processes.

The process begins by loading the scans into greyscale images, the pixel values (which translate to density values) are used to calculate a threshold that is used to define and segment material of interest (seeds).

Due to the low resolution of the imaging technique (68.6\micro meters per pixel) objects can appear connected which are not, a three dimensional watershedding algorithm is used to correct any objects which appear connected when they should not be.

Finally, each isolated object is exported for measuring (objects are defined by groups of connected pixels), measurements are recorded into tables and saved as CSV files. The data is recorded as pixel values and converted to appropriate real-world measurements.

The entire process is represented as a diagram in figure:[[fig:matlab]]

#+NAME: fig:matlab
#+CAPTION: Image Processing Pipeline
#+ATTR_LATEX: :width 15cm
[[file:./images/matlab.png]]

*** Morphometric Features

 The features/phenotypes used are extracted during the imaging process.
 - Length is calculated using the major axis of the whole grain.
 - Width and depth are the major and minor axis of a cross section, found by selecting the grain's midpoint.
 - Volume is a complete connected pixel count per grain
 - Surface area is a single pixel perimeter calculation mapped in 3 dimensions
 - Length X Depth X Width is a post-image-processing value calculated by the interaction between the three dimension descriptors.

 Values used in statistical functions and measurements are presented as metric units, derived from \micro-CT image pixel values. The equation:[[eqn:mm]] is presented here.

 #+NAME: eqn:mm
 \begin{align}
   &\begin{aligned}
 mm = \frac{pixel \times 68.8}{1000}
   \end{aligned}
 \end{align}
 \myequations{Pixel to metric milimeter conversion}


*** Data Analysis Pipeline
The data analysis and research in this project is performed in part by automated software, the CT Analysis Library, and by human decision making as to statistical methods which need to be applied.

Here, the pipeline presented in figure:[[fig:pipeline]] demonstrates the process used by the CT Analysis Library, and by result the CT Analysing GUI.

The first stage of this process is loading in the data exported by the MATLAB software. These data are loaded on a per scan basis, meaning that the previously mentioned process of spike splitting is initially still an issue, a resulting /CT Data/ object is created, this holds the data and carries out the proper treatment for it.

The data needs to be checked for false positives, this is done by first removing outliers which are found by upper and lower percentiles of the data. Additionally constraints are applied to the data based on findings from previous studies cite:Hughes2017, this adds robustness.

When the data has been cleaned, additional data is added. The added data provides information about the experiment, plant names; genotypes; top and bottom scans. With this data, information on scans which need to be joined is provided.

Using data on $x,y,z$ coordinates a approximation is made to join spikes at the most likely point at which they were initially cut for imaging.

An additional phenotype is created to describe the interaction between the geometric parameters; the interaction is described in equation:[[eqn:inter]].

 #+NAME: eqn:inter
 \begin{align}
   &\begin{aligned}
\text{geometry interaction} = length \times depth \times width
   \end{aligned}
 \end{align}
 \myequations{Pixel to metric milimeter conversion}

When data are usable columns can be aggregated to form averages, medians, standard deviations etc. on a per plant (sample) basis.

Finally, a decision is made by way of human input as to what type of statistics should be used to test hypothesis. This decision is mostly based on the distribution of the data where $X \sim \mathcal{N}(\mu,\,\sigma^{2})$ data is tested with parametric tests (e.g. Student's T-Test) else non-parametric tests are used (e.g. Welch's T-Test).

From statistical testing a final result(s) can be used to answer scientific questions on the data. The entire process is represented in figure:[[fig:pipeline]].
\clearpage
#+NAME: fig:pipeline
#+CAPTION: How data is integrated with the CT Analysing Library
#+ATTR_LATEX: :width 18cm
[[file:./images/pipeline.png]]

** Image Analysis Methods
Initially, this project aimed to use existing software for feature extraction, however the range of data (due to the spectrum of genotypes in the study) meant that software which worked well for certain genotypes of wheat did not work well on others.

The issue became in seeds becoming erroneously joined, research and investigation lead to a new solution being implemented in a forked version of the open source software used for image analysis cite:Hughes2017

*** New Watershed Algorithm

In order to solve the problem of misidentified and joint seeds, from the primitive collection,
a  /quasi-euclidean/ distance transform was implemented into the analysis pipeline (figure:[[fig:matlab]]). This provided much better results than the previous
/chessboard/ transform which had been successful on more uniform data in previous studies cite:Hughes2017.

**** Quasi-Euclidean algorithm

This algorithm measures the total euclidean distance along a set of horizontal, vertical and diagonal
line segments cite:Pfaltz1966.

#+NAME: eqn:qe
\begin{align}
  &\begin{aligned}
&|x_1-x_2|+(\sqrt{2}-1)|y_1-y_2|, |x_1-x_2| > |y_1-y_2| \\
      &(\sqrt{2}-1)|x_1-x_2| + |y_1-y_2|, \textup{otherwise.}
  \end{aligned}
\end{align}
\myequations{Quasi-Euclidean Distance}

In order to apply this to a 3D space Kleinberg's method is used  cite:Kleinberg1997. This allows for nearest neighbour pixels to be sorted by $k$ dimensional trees
and enabling fast distance transforms via Rosenfeld and Pfaltz's /quasi-euclidean/ method stated in equation:[[eqn:qe]].

*** Implementation

The software implementation for this was done in MATLAB cite:MATHWORKS2017, this meant that it could instantly and easily be inserted into the existing pipeline in the open source software which was forked and modified to add this feature. In code listing:[[lst:ws]] this new algorithm, in full, is shown.

#+CAPTION: MATLAB Watershedding function
#+LABEL: lst:ws
#+BEGIN_SRC octave -n
  function [W] = watershedSplit3D(A)
    % Takes image stack A and splits it into stack W.
    % This is done by using a watershedding
    % algorithm based on quasi-euclidean distance

    % Convert to BW
    bw = logical(A);
    % Create variable for opening and closing
    se = strel('disk', 5);
    % Minimise object missshapen-ness
    bw = imerode(bw, se);
    bw = imdilate(bw, se);
    % Fill in any left over holes
    bw = imfill(bw,4,'holes');
    % Use chessboard for distance calculation for more refined splitting
    chessboard = -bwdist(~bw, 'quasi-euclidean');
    % Modify the intensity of our bwdist to produce chessboard2
    mask = imextendedmin(chessboard, 2);
    chessboard2 = imimposemin(chessboard, mask);
    % Calculate watershed based on the modified chessboard
    Ld2 = watershed(chessboard2);
    % Take original image and add on the lines calculated for splitting
    W = A;
    W(Ld2 == 0) = 0;
  end
#+END_SRC

The process creates a black and white copy of the image, performs erosion and dilation to remove holes and erroneous pixels. A distance map is then created, this represents each pixel, which is plant material (grains), as an integer value of distance from the closest non-plant pixel. This map combines with the watershed algorithm to provide an estimate of where objects are incorrectly connected (fused grains), subtracting this from the original 3D image provides cleaning split and segmented grains which can be passed through to the next stage of the pipeline.

**** Reasons for Improvement

This improved algorithm for measuring distance between points works well with this data because of the variation in seed morphology.

Where previous studies cite:Hughes2017 found their methods worked well, they also used elite well-bred wheat varieties. These varieties have a more round more even shape with a low standard deviation in shape, generally.

This study, with such a varied group of genotypes has encountered many seeds which do not have a reasonable degree of regularity in shape. As such, a simple distance based method of splitting from the centre of objects was ill suited. By using a quasi-euclidean distance a more flexible shape is afforded in grains and the resulting segmentation is less prone to over-zealous splitting due to non-uniform shape.

**** Effect of Enhanced Watershed algorithm in 2D

Figure:[[fig:qe]] shows in two dimensional flat images how this problem was solved. As well as what the previous method was doing to cause errors in data.
#+CAPTION: /A/ showing the chessboard method, /B/ improved quasi-euclidean method
#+ATTR_LATEX: :width 16cm
#+NAME: fig:qe
[[./images/chess_quasi.png]]

\clearpage
**** Effect of Enhanced Watershed algorithm in 3D

Figure:[[fig:qe3d]] shows the effects on grains in three dimensions, where the seeds which where incorrectly split have now been vastly improved, leading to an increase in result accuracy.

#+CAPTION: /A/ showing before the new method, /B/ after, /C/ zoomed before, /D/ zoomed after.
#+ATTR_LATEX: :width 14cm
#+NAME: fig:qe3d
[[./images/watershedfigure.png]]

\clearpage
** CT Analysing Library Methods
The CT Analysing Library provided by this project, aims to reduce the amount of work and time required to organise, analyse and pull meaning out of \micro-CT data.
Here several of the more novel features of the library are explained.

*** CT Data Object

The CT Analysing Library uses a mixture of object orientated (OOP) and functional programming paradigms, the main initialisation function for the object /CTData/ enables the storing of data and application of functions upon it, making data organisation much more straightforward.

#+CAPTION: CT Data Object Initialisation Method
#+LABEL: lst:ctdata
#+BEGIN_SRC python -n
class CTData():
    def __init__(self, folder, rachis):
	"""
	This is the initialise function for the CTData object,
	this will only need called once.

	@param folder the folder to look for data in
	@param rachis a boolean to decide to load in rachis data or not
	"""
	try:
	    self.make_dataframe(folder, get_rachis=rachis)
	    self.clean_data()
	    self.create_dimensions_ratio()
	    self.df = self.df.reset_index(drop=True)
	except (ValueError, NoDataFoundException):
	    raise NoDataFoundException
	self.additional_data = None
#+END_SRC

\clearpage
*** Loading Data

Initial data loading is performed by the /gather_data/ function, it takes a folder string and performs a regular expression check for all CSV files. From the files found in this search this function performs separation of rachis information (data about the spike's inner nodes) and the files containing seed/grain data.

These are returned together as a tuple, the data found is stored as part of the /CTData/ object for use in the processing pipeline.

#+CAPTION: Loading Data Function
#+LABEL: lst:gather
#+BEGIN_SRC python -n
  def gather_data(self, folder):
      """
      this function gathers together all
      the data of interest

      @param folder is a starting folder
      @returns tuple of (seed files, rachis files)
      """

      # check the end of the folder has a '/'
      if folder[-1] != '/':
	  folder = folder + '/'

      search_params = '{0}*/*.csv'
      candidate_files = glob(search_params.format(folder))

      # Check data was found
      if len(candidate_files) == 0:
	  raise NoDataFoundException

      # we aren't bothered about the raw files so lets remove them
      candidate_files = [f for f in candidate_files if 'raw' not in f]

      # now let's separate out the rachis
      rachis = [f for f in candidate_files if 'rachis' in f]

      # and just assume the rest is what we want
      candidate_files = [f for f in candidate_files if 'rachis' not in f]

      return (candidate_files, rachis)
#+END_SRC

\clearpage
*** Adding Spike Experiment Information

Using a loosely defined Microsoft excel (.xlsx) file, a file of additional information can be loaded into the /CTData/ object. This new information can be any information which is relevant to the individual scans.

This function /get_spike_info/ works by finding the matching column to join data on, then using the provided excel file it adds these new columns of information into the /CTData/ object.

By using \lambda functions (named /look_up/ and /gather_data/) rapid allocation of information can be done in sections, applying hundreds of data entries to the existing data. This is one of many elements in this project which solves challenges in a very functional programming manner.

#+NAME: lst:infojoining
#+CAPTION: Spike Information Joining Algorithm
#+BEGIN_SRC python -n
  def get_spike_info(df, excel_file, join_column='Folder#'):
      """
      This function should do something akin to adding additional
      information to the data frame

      @note there is some confusion in the NPPC about whether to use
      folder name or file name as the unique id when this is made into
      end-user software, a toggle should be added to allow this

      @param excel_file a file to attach and read data from
      @param join_column if the column for joining data is required
      """
      # Grab the linking excel file
      info = pd.read_excel(excel_file,
			   index_col='Folder#')
      # These are the features to grab
      features = list(info.columns)
      # Lambda to look up the feature in excel spreadsheet
      def look_up(x, y): return info.loc[x['folderid']][y]
      # Lambda form a series (data row) and apply it to dataframe
      def gather_data(x): return pd.Series([look_up(x, y) for y in features])
      df[features] = df.apply(gather_data, axis=1)
      # Return the copy
      return df
#+END_SRC

\clearpage
*** Spike Rejoining
The /Scanco/ \micro-CT100 scanner (/Scanco/ Medical, Switzerland) allows for a carousel loading of samples, the maximum allowed length of a sample is ~10cm, in this study the majority of wheat spikes exceed this limit. A solution was found in splitting these samples in half and creating two separated \micro-CT scans.

This process means that post imaging these sample data need to be reconnected and the $z$ axis of all scans need adjusted.

One of the main issues of this task is that there are multiple ways that rejoining could be implemented, this library provides a function that standardises the process. Making repeatable results much easier to produce.

The implementation for this searches through the provided data frame for spikes which are not marked as being 'all', that is to say complete and non-split. From these spikes left, they are either marked 'top' or 'bottom', using an associated 'Sample name' these can be linked. Once linked the top of the spikes need to have their $z$ attribute increased by the size of the bottom spike.

#+CAPTION: Spike Rejoining Function
#+LABEL: lst:join
#+BEGIN_SRC python -n
    def join_spikes_by_rachis(self):
	"""
	Important part of this function is that we accept that the data is what it is
	that is to say: rtop, rbot and Z are all orientated in the proper direction
	It's main purpose is to join split spikes by rachis nodes identified in the
	analysis process

	@param grain_df is the grain dataframe to take on-board
	"""
	# So we are only really interested in grains which are not labelled with
	# 'all' in partition, so let's id them to start with
	for sn in self.df[self.df['Ear'] != 'all']['Sample name'].unique():

	    # Calculate estimated bottom location of a spike
	    bot = self.df.loc[(self.df['Sample name'] == sn)
			      & (self.df['Ear'] == 'bot')]['rbot']

	    # Apply bottom measurement where required by incrementing
	    # 'z' values to top scans
	    self.df.loc[
		(self.df['Sample name'] == sn) &
		(self.df['Ear'] == 'top'), 'z'] =
			self.df.loc[(self.df['Sample name'] == sn) &
			(self.df['Ear'] == 'top'), 'z'] + bot
#+END_SRC

\clearpage
*** Data Aggregating
The analysis method used is focused on extracting data on a per-grain basis, grains have a many to one relationship of spikes.

In order to look at the data on a spike-per-spike basis rather than from seed-to-seed the data needs to aggregated.

The function /aggregate_spike_averages/ takes a list of attributes to be aggregated and a manner of grouping the data. Using this data new columns are created based on median; mean; standard deviation; sum. These new columns are stored in the /CTData/ object directly.

#+CAPTION: Aggregating Data Function
#+LABEL: lst:agg
#+BEGIN_SRC python -n
  def aggregate_spike_averages(self, attributes, groupby):
      """
      This will aggregate features (specified by attributes) into their medians
      on a per-spike basis.

      Makes direct changes to the dataframe (self.df)

      @param attributes list of features to average
      @param groupby how the data should be aggregated
      """

      # Functions which are used as an apply to the dataframe
      trans_funcs = {'median': np.median,
		     'mean': np.mean, 'std': np.std, 'sum': np.sum}

      # For each attribute apply
      for att in attributes:

	  # For each function apply to each attribute in turn
	  for col, func in trans_funcs.items():

	      # Add straight to the dataframe using appropriate formatting to keep
	      # naming straightforward
	      self.df['{0}_{1}'.format(col, att)] = self.df.groupby(groupby)[
		  att].transform(func)

      # Also add some information on the counts of seeds per plant
      self.df['grain_count'] = self.df.groupby(
	  groupby)[groupby].transform(len)
#+END_SRC

\clearpage
*** Cleaning and Removing Erroneous Data

Data are cleaned by calculating percentiles of minimum data, these extreme values are removed. In addition to this, values found in seed/grain literature are used to remove data based on known extremes cite:Gegas2010,Hughes2017.

#+CAPTION: Cleaning Data
#+LABEL: lst:clean_data
#+BEGIN_SRC python -n
  def clean_data(self, remove_small=False, remove_large=False):
      """
      Following parameters outlined in the
      CT software documentation I remove outliers
      which are known to be errors

      @param remove_small a boolean to remove small grains or not
      @param remove_large a boolean to remove larger grains or not
      """
      # Remove all data which has any null or none types
      self.df = self.df.dropna(axis=1, how='all')

      # Remove obviously wrong and too large grains
      self.df = self.df[self.df['surface_area'] < 100]
      self.df = self.df[self.df['volume'] > 3.50]  # this is given for brachy
      self.df = self.df[self.df['volume'] < 60]

      # If the large parameter is set then remove the upper 5th percentile of volume
      if remove_large:
	  self.df = self.df[self.df['volume'] <
			    self.df['volume'].quantile(.95)]
      # If the large parameter is set then remove the lower 5th percentile of volume
      if remove_small:
	  self.df = self.df[self.df['volume'] >
			    self.df['volume'].quantile(.05)]

#+END_SRC

\clearpage
*** Data Transformations (PCA)
In this project several data transformation algorithms are implemented uses the /Scipy/ library cite:Oliphant2007.

For principal component analysis (PCA), the CT Analysis Library implements it using two principal components (PCs) only. The decision to limit this library
to using only two PCs was made to keep data as easy to interpret as possible. Additionally experimentation with this transform shows a high percentage of
coverage and data explanation from only two PCs.

Prior to using the PC function, data are standardised, this is either done through log transforms or by Box Cox power transforms. This provides more normal data and allows for information to be more likely to provide a significant output.

#+CAPTION: PCA Data Transform
#+LABEL: lst:pca
#+BEGIN_SRC python -n
  def perform_pca(df, features, groupby, groupby2=None,
		  groupby3=None, standardise=False):
      """
      This function will perform a PCA and return the principle components as a
      dataframe.

      @param n_components components to check form
      @param df dataframe of the data to analyse
      @param features features from the dataframe to use
      @param groupby the column in the df to use
      @param standardise=False asks whether to standardise the data prior to PCA
      @returns a dataframe of the data, the pca object & the scaled data for reference
      """
      # Only use 2 pcs
      pca = PCA(components=2)
      # Standardise the data if requested
      data = standarise_data(
	  df, features, groupby) if standardise else df.loc[:, features].values
      # Perform pca fitting on the chosen data
      principalComponents = pca.fit_transform(data)
      # form the pca into a dataframe with pca1 and 2 labelled
      principalDf = pd.DataFrame(data=principalComponents, columns=[
				 'principal component 1', 'principal component 2'])
      # Decide how to group the data into the final pca table to return
      # three options are given as multiple analysis may be required
      if groupby2 is None:
	  return (pd.concat([principalDf, df[[groupby]]],
			    axis=1), pca, data)
      if groupby3 is None:
	  return (pd.concat([principalDf, df[[groupby]],
			     df[[groupby2]]], axis=1), pca, data)
      else:
	  return (pd.concat([principalDf, df[[groupby]],
			     df[[groupby2]], df[[groupby3]]], axis=1), pca, data)
#+END_SRC

\clearpage
*** Bayesian Hypothesis Testing

Null-hypothesis significance testing (NHST) is done typically through T-test like functions, where the means of two groups are tested.
As a novel addition, this library provides an improvement upon NHST by using a Bayesian approach proposed by Kruschke in 2012 cite:Kruschke2012.

Here a full model is produced where data are first normalised through log transforms, model parameters set and then 1000 simulations ran via Markov chain Monte Carlo to produce a larger data sample to perform mean testing on.

The 1000 simulations are ran twice, these form two independent chains, checking of convergence and agreement on these random chains is performed to reduce the chance of error.

This function returns a /trace/ object which contains all of the data generated, the data of particular interest is the "difference of means" that this method calculates and the credible interval graphs which it returns the values for, figure:[[fig:ci]] provides an example of such. The example provided shows the likelihood of two means of an interaction term of
$length \times  depth \times width$ overlapping.

#+NAME: fig:ci
#+CAPTION: An example credible interval produced by this method
#+ATTR_LATEX: :width 15cm
[[./images/ci.png]]


The full model is explained later in this chapter in a much more informative manner and fully defined in terms of relevance to results of this project. Listing:[[lst:bayes1]] provides the python model implemented.

#+ATTR_LATEX: :placement [!hpb]
#+LABEL: lst:bayes1
#+CAPTION: Bayesian Model Function
#+BEGIN_SRC python -n
  def baysian_hypothesis_test(group1, group2, group1_name, group2_name):
      """
      Implements and uses the hypothesis test outlined as a robust replacement
      for the t-test
      @param group1 a numpy array to test
      @param group2 a numpy array to test
      @param group1_name the name of the first group
      @param group2_name the name of the second group
      @returns a summary dataframe
      """
      group1 = np.log10(group1)
      group2 = np.log10(group2)
      y = pd.DataFrame(dict(value=np.r_[group1, group2], group=np.r_[
		       [group1_name]*len(group1), [group2_name]*len(group2)]))
      mu_m = y.value.mean()
      mu_s = y.value.std()*2
      with pm.Model() as model:
	  group1_mean = pm.Normal('{0}_mean'.format(group1_name), mu_m, sd=mu_s)
	  group2_mean = pm.Normal('{0}_mean'.format(group2_name), mu_m, sd=mu_s)
      sig_low = 1
      sig_high = 1000
      with model:
	  group1_std = pm.Uniform('{0}_std'.format(
	      group1_name), lower=sig_low, upper=sig_high)
	  group2_std = pm.Uniform('{0}_std'.format(
	      group2_name), lower=sig_low, upper=sig_high)

      with model:
	  nu = pm.Exponential('nu_minus_one', 1/29.) + 1
      with model:
	  lambda_1 = group1_std**-2
	  lambda_2 = group2_std**-2
	  group1 = pm.StudentT(group1_name, nu=nu, mu=group1_mean,
			       lam=lambda_1, observed=group1)
	  group2 = pm.StudentT(group2_name, nu=nu, mu=group2_mean,
			       lam=lambda_2, observed=group2)
      with model:
	  diff_of_means = pm.Deterministic(
	      'difference of means', group1_mean - group2_mean)
	  diff_of_stds = pm.Deterministic(
	      'difference of stds', group1_std - group2_std)
	  effect_size = pm.Deterministic('effect size',
					 diff_of_means /np.sqrt((group1_std**2 +
					 group2_std**2) / 2))
      with model:
	  trace = pm.sample(2000, cores=2)
	  return trace, pm.summary(trace, varnames=['difference of means',
						    'difference of stds','effect size'])
#+END_SRC


\clearpage


** CT GUI Application Methods
The CT GUI Application enables a researcher to perform analysis with a high degree of automation and ease of use. Here several of the elements of software engineering behind this application and its design are explored.

*** Windows
The CT GUI Application uses a tabbed interface, each tab carries out a specific task. This follows Gestalt themes of making user interfaces inherently clear with grouping together functionality in specific areas.

**** The Load Data Window
This window acts as the opening to the software, where a user can select loading of files and data. As well as options such as whether to load spike rachis information.
#+LABEL: fig:loaddata
#+CAPTION: The Initial Data Loading Window for the GUI
#+ATTR_LATEX: :width 11.5cm
[[./images/Screenshots/clean_data_remove_large.png]]
**** Table View
In addition to the tabs provide by the application, a table view is accessed via the system menu bar. This allows an alternative view of the data which is not pictorial.
#+LABEL: fig:loaddata
#+CAPTION: The Table view in used in the GUI
#+ATTR_LATEX: :width 5cm
[[./images/table.png]]
**** The Clean Data Window
The second tab this software introduces is a data cleaning and preparation window. This tab allows additional data to be loaded into an experiment such as scan names, genotype information etc. Options to clean the data by removing outliers is also presented with various options.
#+LABEL: fig:cleandata
#+CAPTION: The Data Cleaning and Loading Window for the GUI
#+ATTR_LATEX: :width 11.5cm
[[./images/clean.png]]

**** The Analysis Window
The third tab is made available once data has been loaded, access to it prior to loading data will cause the application to provide the user with a warning that it will not function correctly without loading information first.

This window gives a user the ability to rapidly investigate data, it enables the plotting of data via histograms and box plots, both of these are incredibly useful for initial investigation of data. It tells researchers a lot about the shape of their data and allows them to see an estimate of means, averages and spread of data.

#+LABEL: fig:analysis
#+CAPTION: The Analysis/Investagation Window for the GUI
#+ATTR_LATEX: :width 11.5cm
[[./images/analysiswindow.png]]

**** The Hypothesis Testing Window
The fourth and final window which this application presents is a method for performing hypothesis testing on the loaded data.

The method for implementing tests is flexible and more could be easily implemented; currently the Student's T-test, Welch's T-test and a Bayesian T-test are implemented to give a range of NHSTs.

#+LABEL: fig:hypotest
#+CAPTION: The Hypothesis Testing Window for the GUI with a difference of means plot
#+ATTR_LATEX: :width 11.5cm
[[./images/differenceofmeanswindow.png]]

#+LABEL: fig:hypotest
#+CAPTION: The Hypothesis Testing Window for the GUI with a T-test and boxplot
#+ATTR_LATEX: :width 11.5cm
[[./images/pvaltest.png]]

\clearpage
*** QT GUI Loading

The GUI is started from a minimal main method, as dictated by standard model-view-controller (MVC) convention. This leads to an /AppWindow/ object which inherits from the /Qt/'s library /QMainWindow/. The MVC model comes into full effect in the constructor for this class.

This constructor assigns a /PyQt5/ GUI file to itself. This allows for a predefined series of objects with rigid layout to be used in the software.

A series of setup functions are called which connect triggers (functions called when GUI elements are interacted with) to the correct controllers.

This main window spawns and hosts the controllers which dictate handling of their respective window (tab).


#+CAPTION: The AppWindow Class' initialisation function
#+LABEL: lst:appwindow
#+BEGIN_SRC python -n
class AppWindow(QMainWindow):
    def __init__(self):
	super().__init__()
	self.ui = Ui_MainWindow()
	self.ui.setupUi(self)
	self.setup_menu_functions()

	# Load in Controllers
	self.find_files_controller = FindFilesWindow(self, self.ui)
	self.analysis_controller = AnalysisWindow(self, self.ui)
	self.ui.master_tab.currentChanged.connect(self.update_analysis_view)
	self.pre_process_controller = PreProcessWindow(self, self.ui)
	self.stats_test_controller = StatsTestWindow(self, self.ui)

	# Set GUI icon for beautification
	self.setWindowIcon(QtGui.QIcon('./images/logo.png'))

	# init some variables
	# Words cannot describe the importance of this object
	self.data = None

	# init states
	self.setup_default_states()
	self.show()
#+END_SRC

\clearpage

*** Connecting Signals from GUI to Function Calls
In example:[[lst:connecting]] lines 11,12,13 show how the UI object has individual members, each of these has a "clicked" trigger, which when triggered calls a function pointer which this controller handles.
#+CAPTION: Example of connecting function pointers
#+NAME: lst:connecting
#+BEGIN_SRC python
  class FindFilesWindow():
      def __init__(self, window, ui):
	  self.ui = ui
	  self.window = window
	  self.connect_view_functions()
      def connect_view_functions(self):
	  # Load data page
	  self.ui.btn_find_files.clicked.connect(self.search_for_files)
	  self.ui.btn_load_data.clicked.connect(self.find_files)
	  self.ui.btn_to_csv.clicked.connect(self.save_file_dialog)
#+END_SRC

*** Integration with CT Analysis Library

The integration with the CT Analysis Library is frequent and spread throughout this application and functions as a key dependency for data organisation.

One example of this is in the load data function shown in example:[[lst:ctguiex]]. Here the custom exception classes are imported and used for data already loaded, as well as the loading functions itself.

#+NAME: lst:ctguiex
#+CAPTION: The load data function from the load_data window
#+BEGIN_SRC python -n
  def find_files(self):
      try:
	  if self.window.get_data() is not None:
	      raise DataAlreadyLoaded
	  self.window.set_data(CTGUIData(self.ui.directory.text(),
					 self.ui.rdb_rachis_yes.isChecked()))
	  if self.window.get_data():
	      self.ui.btn_to_csv.setEnabled(True)
	      self.ui.tab_preprocess.setEnabled(True)
	      self.ui.tab_analysis.setEnabled(True)
	      self.set_files_list()
	      self.ui.lbl_status.setText('Data loaded!')
      except TypeError as e:
	  QMessageBox.warning(self.window, "Finding Files Error",
			      "Couldn't find files in given location")
      except DataAlreadyLoaded as e:
	  QMessageBox.warning(self.window, "Data Already Loaded",
			      "Sorry! You've already loaded in data")
#+END_SRC
*** Plot Window Rendering

Plots where rendered in this application by creating a custom object which inherited from the /matplotlib/ backend /qt5agg/. This object is both a /Qt/ widget and a /matplotlib/ figure. This class provided a straightforward mechanism that would allow for any style of plot (which /matplotlib/ supports) to be created.

This object also acts as a template class for actual implementations, each window type of this application that uses plots (/Analysis Tab/; /Hypothesis Testing Tab/) creates a custom /MyMplCanvas/ child class. To specifically handle inputs which are unique to that window.

The unique inheritance of this class allows it to be treated as a /Qt/ widget, when it is created by a controller it  can be added to the view with no alterations required.

#+CAPTION: The Matplotlib FigureCanvas Polymorphic Class
#+NAME: lst:matbackend
#+BEGIN_SRC python
from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
...
  class MyMplCanvas(FigureCanvas):
      """Ultimately, this is a QWidget (as well as a FigureCanvasAgg, etc.)."""

      def __init__(self, parent=None, window=None, df=None,
		   column=None, width=5, height=4,
		   dpi=100, plot_type=None, group_by=None, ttest=False):

	  if group_by == 'None':
	      self.group_by = None
	  else:
	      self.group_by = group_by
	  self.ttest = ttest
	  self.window = window
	  self.plot_type = plot_type
	  self.column = column

	  if plot_type == 'histogram' and self.group_by is not None:
	      self.fig = self.facet_hist(df, column, group_by)
	      FigureCanvas.__init__(self, self.fig)
	  else:
	      self.fig = Figure(figsize=(width, height), dpi=dpi)
	      self.axes = self.fig.add_subplot(111)
	      FigureCanvas.__init__(self, self.fig)
	      self.compute_initial_figure(self.axes, df)

	  self.setParent(parent)
	  FigureCanvas.setSizePolicy(self,
				     QtWidgets.QSizePolicy.Expanding,
				     QtWidgets.QSizePolicy.Expanding)
	  FigureCanvas.updateGeometry(self)
#+END_SRC

*** Creating GUI Elements When Required

One of the key features of this project, and this GUI application specifically is the ability to handle dynamic input. The loading additional experiment information allows for the opportunity for never-before seen data to be presented. This allows users to define custom classification and testing groups. An example from the research element of this project is shown in figure:[[fig:dynam]].

#+LABEL: fig:dynam
#+CAPTION: Showing how groups are loaded from columns for potential data splitting and testing
#+ATTR_LATEX: :width 10cm
[[./images/dynamicselection.png]]

Here in example:[[lst:testwindowview]] lines 16,17 show that combo buttons are created for the /Hypothesis Testing/ window to display to a user. A similar system is added for the /Analysis Window/ where decisions are carried out as to what columns could be potentially used for object grouping. This is calculated by dividing the columns contents, if it is numeric and has repeating elements or if the contents are text then they are (generally) applicable for use in grouping grains for testing or plotting.

#+NAME: lst:testwindowview
#+CAPTION: The Hypothesis Testing Window class
#+BEGIN_SRC python
  class TestWindowView():
      def __init__(self,df,ui,plot_type):
	  """
	  Given a dataframe and a layout spec
	  this class populates appropriate radio button functionality
	  """
	  self.df = df
	  self.ui = ui
	  self.plot_type = plot_type
	  self.ui.cb_test_grouping.clear()
	  self.ui.cb_test_attribute.clear()
	  # Line up attributes
	  self.ui.cb_test_attribute.addItems(list(
	      filter(lambda x: is_numeric_dtype(df[x]), df.columns)))

	  # Find groups
	  self.ui.cb_test_grouping.addItems(
	      self.find_candidates_for_grouping(self.df))

      def find_candidates_for_grouping(self, df):
	  lst = []
	  for c in df.columns:
	      if len(df[c].unique()) < 30:
		  if not is_numeric_dtype(df[c]):
		      lst.append(c)
	  return lst
#+END_SRC

*** Dynamically Generating Plots
This application provides the ability to make plots instantly, without requiring input from the user to refresh or reload a window.

To do this, the existing canvas window is repainted based on parameter input. Example:[[lst:dynamicplots2]]
shows how this is implemented. A series of conditionals based on UI elements provides data about what data to use and type of plot to use also.

When a hypothesis test is being carried out, this function will call the /CTData/ library to perform the test and return a P-value which is then applied to the plot's title.

#+NAME: lst:dynamicplots2
#+CAPTION: Example code of how figures are computed and implemented using /Seaborn/ and /Matplotlib/
#+BEGIN_SRC python
  def compute_initial_figure(self, axes, df):
      try:
	  if self.plot_type == 'histogram':
	      sns.distplot(df[self.column], ax=self.axes,
			   kde=False, hist_kws=dict(edgecolor="k", linewidth=2))
	  elif self.plot_type == 'boxplot' or self.plot_type == 'welch':
	      sns.boxplot(data=df, x=self.column,
			  y=self.group_by, ax=self.axes)
	      if self.ttest:
		  u = list(df[self.group_by].unique())
		  s1 = df[df[self.group_by] == u[0]][self.column]
		  try:
		      s2 = df[df[self.group_by] == u[1]][self.column]
		  except IndexError:
		      s2 = s1
		  if self.plot_type == 'welch':
		      p = perform_t_test(s1, s2, equal_var=False)
		  else:
		      p = perform_t_test(s1, s2)
		  self.fig.suptitle('P-value of {0}'.format(p))
	      elif self.plot_type == 'bayes':
		  u = list(df[self.group_by].unique())
		  s1 = np.array(df[df[self.group_by] == u[0]][self.column])
		  try:
		      s2 = np.array(df[df[self.group_by] == u[1]][self.column])
		      trace, x = baysian_hypothesis_test(s1, s2, u[0], u[1])
		      plot_difference_of_means(trace, ax=self.axes)
#+END_SRC

\clearpage
*** Creating Facet Plots

To compare data in more than a single dimension the /FacetFrid/ function from the /Seaborn/ library is used. This provides a simple method of creating multiple plots in a single figure.

An example of this is shown when dividing data by ploidy and comparing distributions via histograms in figure:[[fig:facet]].

#+CAPTION: Example of facet plotting by ploidy by grain surface area
#+LABEL: fig:facet
#+ATTR_LATEX: :width 12cm
[[./images/facet_example.png]]

The implementation of /facet plots/ would be simple, however the ability to create sensible plots that organise into a well spaced grid requires additional input.

Example:[[lst:dynamicplots3]] shows on lines 3,5,7 how decisions are made to best arrange the grid. Decisions are made based on the unique number of columns a grouping variable provides. As well this,  additional arguments are  used to indicate further aesthetics of the plots.

#+NAME: lst:dynamicplots3
#+CAPTION: Using Facet wrapping to provide
#+BEGIN_SRC python
  def facet_hist(self, df, column, group_by):
      col_wrap = len(df[group_by].unique()) // 2
      if col_wrap < 4:
	  col_wrap = None
      g = sns.FacetGrid(df, hue=group_by, col=group_by,
			col_wrap=col_wrap)
      g = g.map(sns.distplot, column, kde=False,
		hist_kws=dict(edgecolor="k", linewidth=2))
      g.fig.tight_layout()
      return g.fig
#+END_SRC

** Data Analysis Methods
The research elements of this project went through two distinct phases; the first round of investigation made use of statistical methods most commonly used in similar fields of research. /ANOVA/, Students T-Test and PCA have been heavily used and are well accepted in grain analysis studies cite:Gegas2010,Hughes2017,Xie2015.

Initial findings showed that a more complex model would be required in order to make the best use out of the scientific data which this study produced.

The data which had been gathered, being from such a wide range of samples meant that sample count was relatively low. Distributions of data would also indicate that there were missing elements, with such wide standard deviations.

The solution devised to solve these problems is a Bayesian based model. The choice of this model comes from Bayesian statistics being more and more frequent in biological studies cite:Kruschke2012,Pullen2014. Additionally, a major strength of Bayesian statistics is its ability to adapt to missing data points and make best use of data available.

*** Bayesian Analysis Model

Douglas H. Johnson stated that "Despite their wide use in scientific journals ..., statistical  hypothesis tests add very little value to the products of research" cite:Johnson1999a. In order to answer more complex, more meaningful questions about biological data more detailed and informative approaches are needed, classical methods which involve "p-values" are felt to not be complete solutions cite:Goodman1999.

By moving towards an evidence based model with Bayesian statistics, it is possible to provide interpretable answers, such as “the true parameter \theta has a probability of 0.95 of falling in a 95% credible interval.”

The model provided by this project is an interpretation of the Bayesian T-test proposed by Kruschke in 2012 cite:Kruschke2012.

**** Bayes Theorem

Modern Bayesian interpretations use the simplified model of Bayes original theorem, equation:[[eqn:bayes]] is the basis of the implemented model in this project. This states that: the posterior is proportional to the likelihood times the prior.

 #+NAME: eqn:bayes
 \begin{align}
   &\begin{aligned}
   P(A|B) \propto P(B|A) \times P(A)
   \end{aligned}
 \end{align}
 \myequations{Bayesian Statement}


*** Likelihood
The likelihood is described by the Student's T-test. Where each group $g$ to be tested presents element $y_i$ to a percentage likelihood of fitting a distribution $T$ .

 #+NAME: eqn:likelihood
 \begin{align}
   &\begin{aligned}
y_i^{(g)} \sim T(\nu, \mu, \sigma)
   \end{aligned}
 \end{align}
 \myequations{Likelihood of Model}

**** $\nu$ (Degrees of freedom)
The degrees of freedom used is given as an exponential as described by
Krusche cite:Kruschke2012. This is represented as $\nu = 30$.

 #+NAME: eqn:dof
 \begin{align}
   &\begin{aligned}
\nu = 30
   \end{aligned}
 \end{align}
 \myequations{Degrees of freedom used in Bayesian model}

#+ATTR_LATEX: :width 7cm
#+NAME: fig:expo
#+CAPTION: Exponential Distribution of 30
#+RESULTS:
[[./images/dist2.png]]

**** $\mu$ (Mean)
This is assumed similar for groups $g$. The data are real-values and normal priors are applied (to ensure the posterior is treated similarly).
Twice the standard deviation ensures no values are favoured in the model initially.

 #+NAME: eqn:mu
 \begin{align}
   &\begin{aligned}
\mu_k \sim N(\bar{x},2s)
   \end{aligned}
 \end{align}
 \myequations{Mean equation used in Bayesian model}

#+ATTR_LATEX: :width 7cm
#+NAME: fig:mu
#+CAPTION: Normal$(\bar{x},2s)$
[[./images/mu.png]]


**** $\sigma$ (Standard Deviation)
A large window for standard deviation is used (1,10000). Whilst no values in the model will have this range, it makes no difference due to random sampling via Markov chain Monte Carlo.

 #+NAME: eqn:sd
 \begin{align}
   &\begin{aligned}
\text{Uniform}(1,10000)
   \end{aligned}
 \end{align}
 \myequations{Standard Deviation used in Bayesian model}


#+ATTR_LATEX: :width 7cm
#+NAME: fig:uni
#+CAPTION: Uniform(1,10000)
[[./images/dist.png]]


*** Bayesian Hypothesis Testing Example
A worked example is provided to demonstrate the power of this model.
**** Input Data
Given 20 data points of a simulated set of seed/grain lengths, it is generally not enough information to get a significant result from a T-Test. The data is sparse and not informative.

Figure:[[fig:bayesintro]] provides the simulated data in the form of a histogram where two groups, 'g1' and 'g2' have been used. These are considered the input for the model priors.
#+ATTR_LATEX: :width 7cm
#+NAME: fig:bayesintro
#+CAPTION: Histogram of example input data
[[./images/exampledata.png]]

**** Posterior Output
By applying the method outlined in code example:[[lst:bayes1]] and defined by equation:[[eqn:likelihood]] the posterior predictions can generate 1000 random samples from the perceived distributions of the data. Based on the probability of each value existing within the priors' distribution.

Figure:[[fig:bayeshist]] demonstrates the expanded data set created by the posterior predictions.
#+ATTR_LATEX: :width 7cm
#+NAME: fig:bayeshist
#+CAPTION: Histogram of example posterior data
[[./images/examplebayes.png]]

**** Difference of Means
Using the created data, a simple $\mu^1 - \mu^2$ will provide the difference of group 'g1' and group 'g2' means. Which can be used to interpret the significance likelihood of the groups having different means.

For the provided example data this model states that there is a 25.3% chance that the two means of these groups are different. This is represented in figure:[[fig:bayeshist2]].
#+ATTR_LATEX: :width 7cm
#+NAME: fig:bayeshist2
#+CAPTION: Difference of means plot - $\mu^1 - \mu^2$
[[./images/examplebayes2.png]]



* Results
The research goals behind this project are focused on developing and modelling information on wheat domestication.

With statistical software created specifically to utilise \micro-CT image data on wheat grains, five comparison groups were highlighted for investigation. Two main goals exist in these results, the chosen groupings aim to isolate both:

1. The Effects of ploidy on grain morphology
2. Domestication's influence on grain morphology

Previous studies have used two dimensional morphometric analysis with flatbed scanning systems, Gegas et al. demonstrated through a /Triticum/ mapping population how these data could be used as a framework to partially explain variation in wheat cite:Gegas2010. This project expands by providing new phenotypes previously not examined.

** Bayesian Modelling
The designed Bayesian model previously described has been used as the basis for these results. Here a credible interval of 95% of the data is presented. Null-hypothesis are tested against a difference of means. Data which present >99% chance of different means are considered highly significant, >90% is considered significant (similar to P<0.05 and P<0.01 is used).


** Grain Width is Not Affected by Domestication

The groups /T. beoticum/ and /T. monococcum/ show  that width is significantly different, a 8.6% chance that the mean of /T. beoticum/ width is greater than /T. monococcum/ is reported.

The reported change in width for tetraploids /T. dicoccum/ and /T. dicoccoides/ is not significant. A 36.3% chance exists that the means of the groups are different, which is not a strong difference based on our model.

With one group agreeing to the null-hypothesis and one rejecting it, there is not enough evidence to fully reject that width is associated to solely domestication.

\clearpage
** Grain Length is Not Affected by Domestication

Here, this project has demonstrated that for the diploid genotypes /T. beoticum/ and /T. monococcum/ there is no significant difference in means. The provided model has predicted that there is a 49.6% (figure:\ref{fig:lenmeansdiff}) that these two genotypes are different, this indicates a near perfect overlap of expected data.

#+BEGIN_EXPORT latex
\begin{figure}[!ht]
  \subfloat[\label{subfig-1:length}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group1/length.png}
  }
  \hfill
  \subfloat[\label{subfig-2:Width}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group2/length.png}
  }
  \caption{Boxplots for (a) diploid (2N) wild (red) and domesticated (blue) grains, (b) tetraploid (4N) wild (red) and domesticated (blue) grains}
  \label{fig:lendemest}
\end{figure}
#+END_EXPORT

For the tetraploid genotypes /T. dicoccum/ and /T. dicoccoides/ the effect of domestication appears statistically insignificant. A 27.3% chance is reported as the likelihood of both means being independent. Whilst the data leans towards /T. dicoccum/ having a larger mean, there is insufficient evidence to reject that grain length is not effected by domestication.

Results from diploid and tetraploid genotypes, compared by domestication status, have shown no evidence to suggest that grain length is effected by domestication.


#+BEGIN_EXPORT latex
\begin{figure}[!ht]
  \subfloat[\label{subfig-1:length}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group1/bayes_difference_of_means_length.png}
  }
  \hfill
  \subfloat[\label{subfig-2:Width}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group2/bayes_difference_of_means_length.png}
  }
  \caption{Difference of means plots of wild and domesticated genotypes for length in (a) diploid (2N) grains,(b) tetraploid (4N) grains}
  \label{fig:lenmeansdiff}
\end{figure}
#+END_EXPORT
\clearpage

** Grain Volume Increases with Domestication

For volume, the diploid samples /T. beoticum/, /T. monococcum/  and the tetraploids /T. dicoccum/  /T. dicoccoides/ when internally compared have shown that the difference in domestication is highly significant.

#+BEGIN_EXPORT latex
\begin{figure}[!ht]
  \subfloat[\label{subfig-1:length}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group1/bayes_volume.png}
  }
  \hfill
  \subfloat[\label{subfig-2:Width}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group2/bayes_volume.png}
  }
  \caption{Forest plots showing credible intervals of wild and domesticated genotypes for volume in (a) diploid (2N) grains,(b) tetraploid (4N) grains. (a) shows a 0.4\% chance of similar means  and (b) a 3.6\%}
  \label{fig:lenmeansvolume}
\end{figure}
#+END_EXPORT

The diploids show a 0.4% chance of their means being similar, while the tetraploids show a 3.7% chance of a same mean value.

** Hexaploid Genotypes Contain No Significant Variation

Hexaploids /T. aestivum/ and /T. spelta/ are both elite cultivars. When compared show that there is a 77.2% chance than there is a difference in the means of length. A 54.8% chance of difference in width.

A 91.5% chance and a 89.0% chance of difference of means in volume and length $\times$ depth $\times$ width, respectively.

These values, with the exception of volume, do not provide sufficient evidence of significant variation in hexaploid varieties used in this study.


\clearpage


** Grain Volume Does Not Increase with Ploidy

Diploids (/T. beoticum/, /T. monococcum/) , tetraploids (/T. dicoccum/, /T. dicoccoides/)  show a significant difference in volume.

Hexaploids (/T. aestivum/) when compared to tetraploids (/T. dicoccum/) reports a 86.2% chance of the two means being the same. Indicating that increasing of ploidy fron 4N to 6N does not produce a significant change in grain volume. Figure:[[fig:ploidyvol]] shows this.

#+CAPTION: A boxplot of all grains' volume plotted by ploidy and domestication status
#+NAME: fig:ploidyvol
#+ATTR_LATEX: :width 10cm
[[./images/results/ploidyvol.png]]


Comparing /T. durum/ and /T. aestivum/ there is a 94.7% chance that the means are similar, this indicates that volume is not dependant on ploidy fully.


#+CAPTION: A boxplot of /T. durum/ and /T. aestivum/ volume
#+NAME: fig:ploidyvolume
#+ATTR_LATEX: :width 8cm
[[./images/results/group7/volume.png]]

\clearpage
* Discussion

Here, this chapter will discuss the relevance of this research, its place in the wider field of plant sciences. The meaning of results and their agreement with current literature is also discussed. As well are some interesting findings, which while not statistically significant are cause of interest and further investigation.

** \micro-CT As Tool For Exploring Domestication
In plant science \micro-CT and other imaging techniques have been used to great effect cite:Hubeau2015,Staedler2013,Metzner2015. Here this project builds upon and adds to an established image analysis tool cite:Hughes2017 where this method was designed for drought stress experiments this project has expanded functionality for use with a wide range of genotypes.

A wide range of attributes which previous studies could not access cite:Gegas2010 is made available for study. In figure:[[fig:pairplotexample]] the available phenotypes are displayed in a pairwise fashion, demonstrating how interesting relationships between attributes could provide more information if studied further.

The provided pairwise plots consist of scatter plots, each marker represents a single grain, and diagonals are kernel density estimation plots which show the distribution of the phenotypes. For each comparison group from the study a pairwise plot is provided in supplemental figures.

Primarily, this study has focused on the following phenotypes:

1. Length
2. Width
3. Volume
4. Surface Area

Length and width have previously been studied through two dimensional imaging cite:Gegas2010, their availability through this method provides an interesting point of reference. Volume and surface area both present unstudied phenotypes. Grain studies often use spike weight or thousand-grain-weight (TGW) as an estimation of yield from crops, such large assumptions are unsuited when so much variation exists on a grain-to-grain basis. The relationship between spike weight and sum of spike volume has previously been shown to have been significantly correlated cite:Hughes2017. As such, this study proposes that grain volume is a much more useful measurement, reduces amount of data collection needed and that it can also be used as a proxy for TGW if required.

#+LABEL: fig:pairplotexample
#+CAPTION: A pairplot of two tetraploid genotypes
#+ATTR_LATEX: :width 18cm
[[./images/results/group2/pairplot.png]]

\clearpage

** Effects of Ploidy on Grain Width

Lin Qin et at. reported that an increased grain kernel width was observed in tetraploid genotypes. Furthermore, that this change can be accounted for by /TaGW2/ gene expression levels, where a decline was observed between diploid, tetraploid and hexaploid wheat cite:Qin2017.

It has been suggested that polyploidization events cause stronger effects in grain morphology than domestication or breeding cite:Qin2017. The data made available through this research has the potential to explore and contribute to this suggestion. Where similar effects have been observed but have not been cross compared to answer this question.

However, results from comparing primitive /T. beoticum/ and /T. dicoccoides/ show that an insignificant effect is present and that there is a high likelihood of width overlap. This effect could be down to a poor comparison choice as these genotypes may have arose from separate events cite:Ozkan2002.

** Grain Classification through PCA
Gegas et al. previously made great use of principal component analysis with wheat mapping populations. Using PCA afforded information on grain morphology specifically towards which traits could be traced to features from ancestral species.

With this data, similar solutions could be implemented. Figure:[[fig:grainspca]] shows all grains in this study, grouped by origin spike and plotted by a two component PCA. Input is calculated through averaging across spike grains' length, width, depth, volume and the geometric interaction term, via equation:[[eqn:pca]].

 #+NAME: eqn:pca
 \begin{align}
   &\begin{aligned}
\text{Phenotype}_i = \frac{1}{n}\times\sum^n_{j=1}{x^i_j}
   \end{aligned}
 \end{align}
 \myequations{PCA Equation}


#+LABEL: fig:grainspca
#+CAPTION: A 2 component PCA showing all the spikes discussed in this study
#+ATTR_LATEX: width:15cm
[[./images/pca_all.png]]

** CT Library and CT GUI

The library provided by this project ensures the standardisation and organising of \micro-CT image data for crops. By creating a base line method of processing a lot of repeated work can be avoided. In addition it also provides a new hypothesis testing model which is yet unused in crop science or to study morphology in seeds. The introduction of a easy to use library will hopefully speed up future experiments and make them comparable to previous ones. This contributes to the robustness of the studies themselves and creates work which can be built upon.

The CT GUI application delivers a package in which a user can quickly view vast amounts of data, allowing clear and concise visual representation of thousands of seeds at once. Furthermore, this application gathers data and outputs it in a standard output, this allows for other external software such as /R/-language packages or other /Python/ libraries to make use of it.

Both these pieces of software will be made freely available as research tools, the openness of them, and with the ability for others to modify and contribute to will ensure the ideologies of open science are kept.

Ideally a MIT style software license will be applied to these software packages, and they will also be made available on python library repositories for easy distribution

** Development of Grains
Within the data present exists opportunities yet unexplored. A veritable goldmine of results are waiting to be used to answer questions on grain domestication.

Additional exploration of spike information, grain occurrence relative to position along the spike for example is a hugely interesting trait that has mostly been neglected by other studies. The unique preservation of shape which \micro-CT imaging offers also allows for structure analysis.

There exists the possibility of utilising existing imaging software which explores root patterns in crops to study spike structure cite:Mairhofer2015,Daly2017. This could provide further phenotypic information which may be used to provide explanation in grain development.

Grains have displayed a wide standard deviation in shape and size even within single spikes. This presents an incredibly complex problem that wants exploring, however it is a huge time sink and will be the follow up targets of this work.

* Critical Evaluation
This chapter discusses the project more critically in terms of what worked well, what helped progress, what impeded and also what the limiting factors were.

** Organisational Methods
For organisation of development the system of using weekly sprints (full diary in appendix:[[sec:appendix]]) worked very well. The project being very research heavy, exploring lots of unknowns, created a lot of uncertainty. Had a waterfall methodology been used, for example, adaption to new information would have been very difficult and required a huge amount of refactoring of documentation.

Reflective diaries of progress and ideas being shared with supervisors and other researchers allowed for very rapid information sharing, a slight reduction in formality with the huge boon of efficiency. Invoking the agile mentality of interactions over processes.

** Relevance to Degree
The project was carried out as part of completion of a BSc degree in Computer Science. It builds upon many of modules which were studied and they were also able to provide a basis of information and knowledge:

*** MA35210 - Topics In Biological Statistics
This module provided a firm understanding of how statistics can be used in biological systems. In particular the knowledge gained from linear modelling, regression analysis and principal component analysis was incredibly useful for this project, and enhanced it greatly.
*** CS31310 - Agile Methodologies
The development practices used in this project are heavily inspired by the agile manifesto. Learning how to interact with clients, use SCRUM and weekly sprint techniques were pivotal to the project's progress over the course of development.
*** CS34110 - Computer Vision
The computer vision module provided the basis of image analysis techniques, a set of tools and which to use to solve certain problems. Semantic segmentation techniques in particular were used extensively throughout the image analysis pipeline.
*** CS36110 - Machine Learning
The data analysis model used in the research methods is heavily based on Bayesian machine learning techniques. The machine learning module provided key insight on Bayes theorem.
*** CS27020 - Modelling Persistent Data
The data model used here is loosely defined to allow for change in experimental parameters, the ideas behind it come from "NoSQL" style data storage systems. Flat structures which have become incredible adaptable in terms of modelling.
*** CSS0160 - Industrial year
The source of this work, and reason for it being carried out are due to work performed on the industrial year scheme. An internship at the National Plant Phenomics Centre provided the insight into plant biology and its relationship with computer science, statistics and high-throughput experiments.

The background biological knowledge required for this project was founded here.

** Time Management
Major goals where met on time, well in advance to allow for change when required. The project started with a lot of ambition to explore the data fully. Unfortunately time and deadlines allowed for only the key elements of polyploidization and domestication of grain to be investigated.

Despite the pull of interesting data findings, the work of this project has been completed on time without distraction. Although the expected work, as always, was much larger than initially forecast no issues where caused.

However, if more time was given certain elements of the software may have been revised. Further usage by researchers of the library and GUI application may have uncovered  different and more useful methods that could have been factored in.

** Original Aims
In terms of software deliverable the project met all of those outlined. In particular some of the FRs are implemented in ways which are straightforward and would be difficult to improve; F.R:5 implements a method in the CT Analysing Library for rejoining spikes which were split. This method achieves and meets the requirements quite succinctly, the implementation is lean and to the point.

In contrast, F.R:4 is one of the more loosely defined requirements, stating that hypothesis testing should be performed by the library. This was difficult to fully implement for a few reasons.

1. It was wasn't clearly defined.
2. Hypothesis testing has hundreds of valid approaches all depending on the experiment and input data.
3. Creating a GUI to allow multiple hypothesis tests would require many settings to be changed and tweaked which could become too obscured and difficult to use.

To accommodate for this two of the most widely used tests in biological research were implemented, the Student's T-test and Welch's T-test. In addition the specific Bayesian estimation hypothesis test that was used to answer the research questions of the project was implemented.


** Quality of Work
The quality of the work done had been to a high standard, as one of the central dogmas of research is robustness. A higher output of work could have been achieved if quality was scarified. For example a custom data model being devised and researched took a large amount of time and effort, existing solutions could have been used and allow more hypothesis to be asked and answered. The results however, would be much less useful.

The software provided by this project is built to be extensible, it presents as much a framework for "how" tasks should be carried out as it does actually implement tasks. This means that in future experiments new tests could be added to the hypothesis testing library, new data formats could be added to the data exporter and additional phenotypes could easily be included in the analysis window.

** Further Work

As mentioned, the time constraint imposed upon this project prevented a full exploration of the data created. It is proposed that many more interesting results could be drawn from it. Development of spikes, exploring the formation of grain florets and fertilisation processes are of great interest (why some grains become fully formed and others are left not fully formed).

Additional groupings, such as into floret subgroups could provide a new and interesting way to compare grains. It would allow the study of individual spikes, divided by sub-regions. Further investigation in this area may provide critical information as to why specific regions of spikes have more/larger grains whilst others do not fully fill their potential.

Contributing more tests to the CT Analysis Library would be a main goal of future work also. The distributions and style of the data presented may require thinking about in new and novel ways. This could be interesting.

Finally, creating a repository for seed morphometrics will be a goal for further work, contributing too and allowing other researchers to add to a centralised library. Doing this could create a large multi-experiment analysis to take place with standardised data.


\appendix

* Glossary
#+ATTR_LATEX: :environment tabularx :width \textwidth :align |l|X|
#+NAME: tab:glossary
#+CAPTION: Dictionary for Terms and acronyms
|----------------------------------------+------------------------------------------------------------------------------|
| *Term*                                 | *Definition*                                                                 |
|----------------------------------------+------------------------------------------------------------------------------|
| \micro-CT                              | Micro Computed Tomography                                                    |
|----------------------------------------+------------------------------------------------------------------------------|
| Genotype                               | A genetically distinct individual or group                                   |
|----------------------------------------+------------------------------------------------------------------------------|
| Phenotype                              | A physical/measurable trait                                                  |
|----------------------------------------+------------------------------------------------------------------------------|
| Alleles                                | A variant of a gene                                                          |
|----------------------------------------+------------------------------------------------------------------------------|
| Genus                                  | Classification ranking, below the /family/ grouping                          |
|----------------------------------------+------------------------------------------------------------------------------|
| Genome                                 | The complete genetic make up of an organism, which defines its individuality |
|----------------------------------------+------------------------------------------------------------------------------|
| Morphometric                           | The shape and form of an organism                                            |
|----------------------------------------+------------------------------------------------------------------------------|
| GUI                                    | Graphical User Interface                                                     |
|----------------------------------------+------------------------------------------------------------------------------|
| PCA                                    | Principal Component Analysis                                                 |
|----------------------------------------+------------------------------------------------------------------------------|
| Spike                                  | A singular stalk of wheat                                                    |
|----------------------------------------+------------------------------------------------------------------------------|
| Spikelet                               | A group of seeds all forming from the same node in a spike                   |
|----------------------------------------+------------------------------------------------------------------------------|
| MVC                                    | Model View Controller - A design pattern for GUIs                            |
|----------------------------------------+------------------------------------------------------------------------------|
| OOP                                    | Object Orientated Programming                                                |
|----------------------------------------+------------------------------------------------------------------------------|
| $X \sim \mathcal{N}(\mu,\,\sigma^{2})$ | Notation for Normal (Gaussian) Distribution                                   |
|----------------------------------------+------------------------------------------------------------------------------|

\clearpage


* Libraries, Tools and Documentation

*** Libraries
#+ATTR_LATEX: :environment tabularx :width \textwidth :align |X|X|X|
#+NAME: tab:software
#+CAPTION: Software libraries used
|---------------------------------+-------+------------|
| Seaborn                         | Scipy | Sklearn    |
|---------------------------------+-------+------------|
| MATLAB Image Processing Toolbox | Numpy | Matplotlib |
|---------------------------------+-------+------------|
| Statsmodels                     | Pymc3 | Xlrd       |
|---------------------------------+-------+------------|
| PyQt5                           | gcc   | Pip        |
|---------------------------------+-------+------------|

*** Tools
#+ATTR_LATEX: :environment tabularx :width \textwidth :align |X|X|X|
#+NAME: tab:softwareused
#+CAPTION: Software tools used
|--------+-----------------------+----------|
| MATLAB | Python Debugger (PDB) | IPython  |
|--------+-----------------------+----------|
| Emacs  | git                   | org-mode |
|--------+-----------------------+----------|
| Tomviz | ImageJ                | PlantUML |
|--------+-----------------------+----------|

** CT Analysis Library Documentation
\clearpage

#+BEGIN_EXPORT latex

\includepdf[scale=0.8,pages=1,frame,pagecommand={\subsecton{CT Analysis Library Documentation Page 1}}]{ctanalysis.pdf}
\includepdf[scale=0.8,pages=2,frame,pagecommand={\subsecton{CT Analysis Library Documentation Page 2}}]{ctanalysis.pdf}
\includepdf[scale=0.8,pages=3,frame,pagecommand={\subsecton{CT Analysis Library Documentation Page 3}}]{ctanalysis.pdf}
\includepdf[scale=0.8,pages=4,frame,pagecommand={\subsecton{CT Analysis Library Documentation Page 4}}]{ctanalysis.pdf}
\includepdf[scale=0.8,pages=5,frame,pagecommand={\subsecton{CT Analysis Library Documentation Page 5}}]{ctanalysis.pdf}
\includepdf[scale=0.8,pages=6,frame,pagecommand={\subsecton{CT Analysis Library Documentation Page 6}}]{ctanalysis.pdf}
\includepdf[scale=0.8,pages=7,frame,pagecommand={\subsecton{CT Analysis Library Documentation Page 7}}]{ctanalysis.pdf}
\includepdf[scale=0.8,pages=8,frame,pagecommand={\subsecton{CT Analysis Library Documentation Page 8}}]{ctanalysis.pdf}
\includepdf[scale=0.8,pages=9,frame,pagecommand={\subsecton{CT Analysis Library Documentation Page 9}}]{ctanalysis.pdf}
\includepdf[scale=0.8,pages=10,frame,pagecommand={\subsecton{CT Analysis Library Documentation Page 10}}]{ctanalysis.pdf}

#+END_EXPORT

** CT Analysis GUI User Walk Through


#+BEGIN_EXPORT latex
\includepdf[scale=0.8,pages=3,frame,pagecommand={\subsecton{CT Analysis Library Documentation Page 1}}]{examplegui.pdf}
\includepdf[scale=0.8,pages=4,frame,pagecommand={\subsecton{CT Analysis Library Documentation Page 2}}]{examplegui.pdf}
\includepdf[scale=0.8,pages=5,frame,pagecommand={\subsecton{CT Analysis Library Documentation Page 3}}]{examplegui.pdf}
#+END_EXPORT



* Wheat Varieties
#+ATTR_LATEX: :environment tabularx :width \textwidth :align |X|X|
#+NAME: tab:wheat
#+CAPTION: Wheat used in this work and their common names
|------------------------+---------------------|
| *Species name*         | *Common name*       |
|------------------------+---------------------|
| /Triticum monococcum/  | Einkorn Domesticate |
|------------------------+---------------------|
| /Triticum boeticum/    | Einkorn Wild        |
|------------------------+---------------------|
| /Triticum durum/       | Pasta Wheat         |
|------------------------+---------------------|
| /Triticum dicoccoides/ | Emmer Domesticate   |
|------------------------+---------------------|
| /Triticum dicoccum/    | Emmer Wild          |
|------------------------+---------------------|
| /Triticum ispahanicum/ | n/a                 |
|------------------------+---------------------|
| /Triticum timopheevii/ | n/a                 |
|------------------------+---------------------|
| /Triticum spelta/      | Spelt               |
|------------------------+---------------------|
| /Triticum aestivum/    | Bread Wheat         |
|------------------------+---------------------|
| /Triticum compactum/   | Club Wheat          |
|------------------------+---------------------|



\clearpage
* Code Segments and Examples

** Setup.py
#+CAPTION: The /setup.py/ configuration for the CT Analyser Library
#+LABEL: lst:setuppy
#+BEGIN_SRC python -n
  from setuptools import setup
  setup(name='CT_Analysing_Library',
	version='0.2',
	description='Library used for CT grain analysis at the NPPC',
	url='https://github.com/SirSharpest/CT_Analysing_Library',
	author='Nathan Hughes',
	author_email='nathan1hughes@gmail.com',
	license='MIT',
	packages=['ct_analysing_library'],
	install_requires=['pandas',
			  'numpy',
			  'matplotlib',
			  'seaborn',
			  'scipy',
			  'sklearn',
			  'statsmodels',
			  'pymc3',
			  'xlrd'],
	zip_safe=True)
#+END_SRC

\clearpage
** Self-Documenting Code Example
#+CAPTION: Example of code documentation and readability from /data_transforms.py/
#+LABEL: lst:docexample
#+BEGIN_SRC python -n
  def get_spike_info(self, excel_file, join_column='Folder#'):
      """
      This function should do something akin to adding additional
      information to the data frame

      @note there is some confusion in the NPPC about whether to use
      folder name or file name as the unique id when this is made into
      end-user software, a toggle should be added to allow this

      @param excel_file a file to attach and read data from
      @param join_column if the column for joining data is
      different then it should be stated
      """
	  # Grab the linking excel file
	  info = pd.read_excel(excel_file,
			       index_col='Folder#')

	  features = list(info.columns)
	  # Lambda to look up the feature in excel spreadsheet
	  def look_up(x, y): return info.loc[x['folderid']][y]

	  # Lambda form a series (data row) and apply it to dataframe
	  def gather_data(x): return pd.Series(
	      [look_up(x, y) for y in features])

	  self.df[features] = self.df.apply(gather_data, axis=1)
      except KeyError as e:
	  print('Error matching data')
	  print(e)
	  raise NoDataFoundException
      except AttributeError as e:
	  print(e)
	  raise NoDataFoundException

#+END_SRC
\clearpage

** Custom Documentation Generator
#+CAPTION: Custom lisp code for generating easy to read documentation
#+LABEL: lst:docgen
#+BEGIN_SRC emacs-lisp -n
  (defun populate-org-buffer (buffer filename root)
    (goto-char (point-min))
    (let ((to-insert (concat "* " (replace-regexp-in-string root "" filename) "\n") ))
      (while (re-search-forward
	      (rx (group (or "def" "class"))
		  space
		  (group (+ (not (any "()"))))
		  (? "(" (* nonl) "):" (+ "\n") (+ space)
		     (= 3 "\"")
		     (group (+? anything))
		     (= 3 "\"")))
	      nil 'noerror)
	(setq to-insert
	      (concat
	       to-insert
	       (if (string= "class" (match-string 1))
		   "** "
		 "*** ")
	       (match-string 2)
	       "\n"
	       (and (match-string 3)
		    (concat (match-string 3) "\n")))))helm-semantic-or-imenu
      (with-current-buffer buffer
	(insert to-insert))))
  (defun org-documentation-from-dir (&optional dir)
    (interactive)
    (let* ((dir  (or dir (read-directory-name "Choose base directory: ")))
	   (files (directory-files-recursively dir "\py$"))
	   (doc-buf (get-buffer-create "org-docs")))
      (dolist (file files)
	(with-temp-buffer
	  (insert-file-contents file)
	  (populate-org-buffer doc-buf file dir)))
      (with-current-buffer doc-buf
	(org-mode))))
#+END_SRC

\clearpage

* Bayesian Results

** /T. monococcum/ and /T. beoticum/
#+ATTR_latex: :environment longtable :align l|l|l
|   | Phenotype          | Less than probability |
|---+--------------------+-----------------------|
|   | length_depth_width |              0.000625 |
|   | length             |              0.495875 |
|   | width              |              0.083125 |
|   | surface_area       |               0.02225 |
|   | depth              |                0.0325 |
|   | volume             |               0.00425 |

** /T. dicoccum/ and /T. dicoccoides/
#+ATTR_latex: :environment longtable :align l|l|l
|   | Phenotype          | Less than probability |
|---+--------------------+-----------------------|
|   | length_depth_width |              0.030375 |
|   | length             |              0.260375 |
|   | width              |              0.363125 |
|   | surface_area       |                  0.35 |
|   | depth              |               0.24325 |
|   | volume             |                0.0385 |

** /T. spelta/ and /T. aestivum/
#+ATTR_latex: :environment longtable :align l|l|l
|   | Phenotype          | Less than probability |
|---+--------------------+-----------------------|
|   | length_depth_width |              0.887125 |
|   | length             |              0.760875 |
|   | width              |               0.52075 |
|   | surface_area       |              0.723375 |
|   | depth              |               0.60775 |
|   | volume             |                 0.918 |


\clearpage
** /T. beoticum/ and /T. dicoccoides/
#+ATTR_latex: :environment longtable :align l|l|l
|   | Phenotype          | Less than probability |
|---+--------------------+-----------------------|
|   | length_depth_width |              0.887125 |
|   | length             |              0.760875 |
|   | width              |               0.52075 |
|   | surface_area       |              0.723375 |
|   | depth              |               0.60775 |
|   | volume             |                 0.918 |

** /T. dicoccum/ and /T. durum/
#+ATTR_latex: :environment longtable :align l|l|l
|   | Phenotype          | Less than probability |
|---+--------------------+-----------------------|
|   | length_depth_width |              0.333125 |
|   | length             |                 0.806 |
|   | width              |                 0.238 |
|   | surface_area       |              0.306125 |
|   | depth              |              0.292125 |
|   | volume             |                 0.343 |

** /T. dicoccum/ and /T. aestivum/
#+ATTR_latex: :environment longtable :align l|l|l
|   | Phenotype          | Less than probability |
|---+--------------------+-----------------------|
|   | length_depth_width |              0.894625 |
|   | length             |                0.9115 |
|   | width              |              0.432125 |
|   | surface_area       |              0.628125 |
|   | depth              |              0.502125 |
|   | volume             |                 0.864 |

** /T. durum/ and /T. aestivum/
#+ATTR_latex: :environment longtable :align l|l|l
|   | Phenotype          | Less than probability |
|---+--------------------+-----------------------|
|   | length_depth_width |              0.970875 |
|   | length             |              0.741375 |
|   | width              |                 0.691 |
|   | surface_area       |              0.812375 |
|   | depth              |                 0.711 |
|   | volume             |               0.94725 |


* Full Comparison Plots

** /T. monococcum/ and /T. beoticum/
*** Data Distributions

#+BEGIN_EXPORT latex
\begin{figure}[!ht]
  \subfloat[Boxplot of \textit{T. monococcum} and \textit{T. beoticum} Length  \label{subfig-1:length}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group1/length.png}
  }
  \hfill
  \subfloat[Boxplot of \textit{T. monococcum} and \textit{T. beoticum} Width  \label{subfig-2:Width}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group1/width.png}
  }
  \hfill
  \subfloat[Boxplot of \textit{T. monococcum} and \textit{T. beoticum} Volume  \label{subfig-3:volume}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group1/volume.png}
  }
  \hfill
  \subfloat[Boxplot of \textit{T. monococcum} and \textit{T. beoticum} Length X depth X width \label{subfig-4:lengthwidthdepth}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group1/length_depth_width.png}
    }
  \caption{Plots comparing means and distributions of \textit{T. monococcum} (red) and \textit{T. beoticum} (blue)}
  \label{fig:dummy}
\end{figure}
#+END_EXPORT
\clearpage
*** Pair Plot

#+LABEL:
#+CAPTION:
#+ATTR_LATEX: :width 18cm
[[./images/results/group1/pairplot.png]]

\clearpage
** /T. dicoccum/ and /T. dicoccoides/

*** Data Distributions

#+BEGIN_EXPORT latex
\begin{figure}[!ht]
  \subfloat[Boxplot of \textit{T. dicoccum} and \textit{T. dicoccoides} Length  \label{subfig-1:length}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group2/length.png}
  }
  \hfill
  \subfloat[Boxplot of \textit{T. dicoccum} and \textit{T. dicoccoides} Width  \label{subfig-2:Width}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group2/width.png}
  }
  \hfill
  \subfloat[Boxplot of \textit{T. dicoccum} and \textit{T. dicoccoides} Volume  \label{subfig-3:volume}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group2/volume.png}
  }
  \hfill
  \subfloat[Boxplot of \textit{T. dicoccum} and \textit{T. dicoccoides} Length X depth X width \label{subfig-4:lengthwidthdepth}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group2/length_depth_width.png}
    }
  \caption{Plots comparing means and distributions of \textit{T. dicoccum} (red) and \textit{T. dicoccoides} (blue)}
  \label{fig:dummy}
\end{figure}
#+END_EXPORT
\clearpage
*** Pair Plot

#+LABEL:
#+CAPTION:
#+ATTR_LATEX: :width 18cm
[[./images/results/group2/pairplot.png]]

\clearpage
** /T. spelta/ and /T. aestivum/

*** Data Distributions

#+BEGIN_EXPORT latex
\begin{figure}[!ht]
  \subfloat[Boxplot of \textit{T. spelta} and \textit{T. aestivum} Length  \label{subfig-1:length}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group3/length.png}
  }
  \hfill
  \subfloat[Boxplot of \textit{T. spelta} and \textit{T. aestivum} Width  \label{subfig-2:Width}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group3/width.png}
  }
  \hfill
  \subfloat[Boxplot of \textit{T. spelta} and \textit{T. aestivum} Volume  \label{subfig-3:volume}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group3/volume.png}
  }
  \hfill
  \subfloat[Boxplot of \textit{T. spelta} and \textit{T. aestivum} Length X depth X width \label{subfig-4:lengthwidthdepth}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group3/length_depth_width.png}
    }
  \caption{Plots comparing means and distributions of \textit{T. spelta} (red) and \textit{T. aestivum} (blue)}
  \label{fig:dummy}
\end{figure}
#+END_EXPORT
\clearpage
*** Pair Plot

#+LABEL:
#+CAPTION:
#+ATTR_LATEX: :width 18cm
[[./images/results/group3/pairplot.png]]

\clearpage
** /T. beoticum/ and /T. dicoccoides/

*** Data Distributions

#+BEGIN_EXPORT latex
\begin{figure}[!ht]
  \subfloat[Boxplot of \textit{T. beoticum} and \textit{T. dicoccoides} Length  \label{subfig-1:length}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group4/length.png}
  }
  \hfill
  \subfloat[Boxplot of \textit{T. beoticum} and \textit{T. dicoccoides} Width  \label{subfig-2:Width}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group4/width.png}
  }
  \hfill
  \subfloat[Boxplot of \textit{T. beoticum} and \textit{T. dicoccoides} Volume  \label{subfig-3:volume}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group4/volume.png}
  }
  \hfill
  \subfloat[Boxplot of \textit{T. beoticum} and \textit{T. dicoccoides} Length X depth X width \label{subfig-4:lengthwidthdepth}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group4/length_depth_width.png}
    }
  \caption{Plots comparing means and distributions of \textit{T. beoticum} (red) and \textit{T. dicoccoides} (blue)}
  \label{fig:dummy}
\end{figure}
#+END_EXPORT
\clearpage
*** Pair Plot

#+LABEL:
#+CAPTION:
#+ATTR_LATEX: :width 18cm
[[./images/results/group4/pairplot.png]]

\clearpage
** /T. dicoccum/ and /T. durum/

*** Data Distributions

#+BEGIN_EXPORT latex
\begin{figure}[!ht]
  \subfloat[Boxplot of \textit{T. dicoccum} and \textit{T. durum} Length  \label{subfig-1:length}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group5/length.png}
  }
  \hfill
  \subfloat[Boxplot of \textit{T. dicoccum} and \textit{T. durum} Width  \label{subfig-2:Width}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group5/width.png}
  }
  \hfill
  \subfloat[Boxplot of \textit{T. dicoccum} and \textit{T. durum} Volume  \label{subfig-3:volume}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group5/volume.png}
  }
  \hfill
  \subfloat[Boxplot of \textit{T. dicoccum} and \textit{T. durum} Length X depth X width \label{subfig-4:lengthwidthdepth}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group5/length_depth_width.png}
    }
  \caption{Plots comparing means and distributions of \textit{T. dicoccum} (red) and \textit{T. durum} (blue)}
  \label{fig:dummy}
\end{figure}
#+END_EXPORT
\clearpage
*** Pair Plot

#+LABEL:
#+CAPTION:
#+ATTR_LATEX: :width 18cm
[[./images/results/group5/pairplot.png]]


\clearpage

** /T. dicoccum/ and /T. aestivum/

*** Data Distributions

#+BEGIN_EXPORT latex
\begin{figure}[!ht]
  \subfloat[Boxplot of \textit{T. dicoccum} and \textit{T. aestivum} Length  \label{subfig-1:length}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group6/length.png}
  }
  \hfill
  \subfloat[Boxplot of \textit{T. dicoccum} and \textit{T. aestivum} Width  \label{subfig-2:Width}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group6/width.png}
  }
  \hfill
  \subfloat[Boxplot of \textit{T. dicoccum} and \textit{T. aestivum} Volume  \label{subfig-3:volume}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group6/volume.png}
  }
  \hfill
  \subfloat[Boxplot of \textit{T. dicoccum} and \textit{T. aestivum} Length X depth X width \label{subfig-4:lengthwidthdepth}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group6/length_depth_width.png}
    }
  \caption{Plots comparing means and distributions of \textit{T. dicoccum} (red) and \textit{T. aestivum} (blue)}
  \label{fig:dummy}
\end{figure}
#+END_EXPORT
\clearpage
*** Pair Plot

#+LABEL:
#+CAPTION:
#+ATTR_LATEX: :width 18cm
[[./images/results/group6/pairplot.png]]

\clearpage
** /T. durum/ and /T. aestivum/

*** Data Distributions

#+BEGIN_EXPORT latex
\begin{figure}[!ht]
  \subfloat[Boxplot of \textit{T. durum} and \textit{T. aestivum} Length  \label{subfig-1:length}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group7/length.png}
  }
  \hfill
  \subfloat[Boxplot of \textit{T. durum} and \textit{T. aestivum} Width  \label{subfig-2:Width}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group7/width.png}
  }
  \hfill
  \subfloat[Boxplot of \textit{T. durum} and \textit{T. aestivum} Volume  \label{subfig-3:volume}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group7/volume.png}
  }
  \hfill
  \subfloat[Boxplot of \textit{T. durum} and \textit{T. aestivum} Length X depth X width \label{subfig-4:lengthwidthdepth}]{%
    \includegraphics[width=0.45\textwidth]{./images/results/group7/length_depth_width.png}
    }
  \caption{Plots comparing means and distributions of \textit{T. durum} (red) and \textit{T. aestivum} (blue)}
  \label{fig:dummy}
\end{figure}
#+END_EXPORT
\clearpage
*** Pair Plot

#+LABEL:
#+CAPTION:
#+ATTR_LATEX: :width 18cm
[[./images/results/group7/pairplot.png]]

\clearpage


* Sprint Timeline
  <<sec:appendix>>

*** Sprint - Week 0
Initial planning was taken out, discussions with researchers at the National Plant Phenomics Centre (NPPC), to create a general set of targets and research goals.

A website was built in order to host weekly progress reports, this was used to share with supervisors and with staff at the NPPC. It also provided a list of discussion points to go through at weekly meetings.

A bug was identified in image analysis software, this was raised to be fixed a later date.

A literature review was taken out to highlight the novelty of this research, as well as current trends in the field in terms of analysis and known/accepted information.

*** Sprint - Week 1

Initial running of grain analysis software was performed multiple times, as per instructions in literature cite:Hughes2017, multiple parameters for minimum and maximum expected sizes of grains needed to be tested. Data which was produced was very noisy and would require further work.

Spike work was carried out in investigating the potential of using a skeletonising method on the wheat spikes. The hope behind this was to simplify structure in a three dimensional structure, of 1 pixel thick lines. This technique is often used in plant root analysis cite:Mairhofer2015,Daly2017. Experimentation with these methods were technically challenging and a decision was made to revisit if time permitted at the end of the project.

A key function of the CT Analysis Library was created, showing in listing:[[lst:infojoining]]. The method enables additional experiment information to be joined with extracted seed data. This provides a way of grouping seeds into more useful groupings than just their scanning data.

An issue was identified in the choices of testing which are typically used in these studies, the use of ANOVA and Student's T-Test, for example, are best used with parametric data, that is to say data where distribution is normally distributed. Further reading into this presented the use of Box Cox transforms as a method to counter these issues.

An issue was raised; a method for visualising outliers in the data could provide greatly beneficial insight into finding errors. If time was available at the end of the project, this would be explored further.

*** Sprint - Week 2

Spikes of wheat are scanned in two separate imaging cycles sometimes, this is because the tube used by the \micro-CT machine are 10cm tall and often a spike will exceed this. In order for full analysis to be carried out these separate scans need to be rejoined. A method for doing this was added to the python Analysis Library.

An initial Model-View-Controller (MVC) model was constructed for how a GUI might take form around. Using this several wire-frames were created

An idea for how data could be cleaned was created by using the information found in previous studies cite:Hughes2017. From this reported data minimum and maximum expected size could be assumed for wheat grains in terms of volume.

Decisions to move the CT Grain Analysing Library towards an object orientated model were made during this sprint, after evaluating the potential of a functionally programmed model or a object one. Handling everything in terms of classes was decidedly easier in terms of understanding how to use the library.

*** Sprint - Week 3

A lot of progress was made on constructing a GUI here, dynamic plotting was put together as a proof of this concept. Histograms of the data were able to be made by using the mouse to select which attribute to measure.

GUI elements such as navigation, data tabs and file menus were added to make more clear to the user how functionality should work.

A novel and new method for generating documentation was created specifically for documenting this software library. Using a plain text format, doc strings from python code was used to make a single easy to read PDF to be distributed with the software package.

Another piece of functionality was introduced to the GUI that would enable the user to view the data they had loaded as a single 2 dimensional data frame. This is for quickly viewing data as a single source and all together. This had arisen as a functionality desired by researchers in a previous weeks meeting.

*** Sprint - Week 4
A new method for watershedding was developed and deployed in the data extraction pipeline, this moved towards using euclidean distance transforms. Allowing for the more complex shapes which this data set presented as a problem to be fully and properly segmented.

Further work was done in terms of the GUI to enable matplotlib and seaborn library integration with the dynamic plotting features which were developed previously.

Custom exceptions were written for the CT Analysing Library, these exceptions allow for more detailed feedback to a programmer when they perform actions which might result in erroneous data processing. In particular a "NoDataFoundException" was made for when empty data files where found in searching.

Current versions of doc strings were enhanced to match with newer functionality which had been added in recent weeks, and documentation was regenerated.

*** Sprint - Week 5
Functions were added to the CT Analysing Library which would allow for aggregation of data columns, this meant that averaging functions could be performed on a per-spike basis. Mean, standard deviation, sum and count functions were applied by default.

Further research was carried out into data transforms which are commonly used in data science; Principal component analysis, linear modelling and multivariate analysis were topics researched as possibilities for inclusion in this library.

*** Sprint - Week 6
Preparation for a mid-project demo took up the majority of this sprint, ensuring data which had been gathered to date was well understood was vital.

A presentation was created using data extracted via the CT Analysing Library, this was in the mode of figures and graphs which could then be used to better explain the theme of the project.

*** Sprint - Week 7
An initial implementation of principal component analysis was tested and found to be useful, a considerable amount of time was dedicated to refactoring the code-base to allow for new data transforms to be easily incorporated.

Additions to the graphing section of the library also allowed for splitting data into multiple plots which could then be featured side-by-side, separation of data helped contribute to seeing the differences in the data.

Some initial ideas for unit testing were written up, an investigation into which platform and library were best to use was carried out. This had to be taken into consideration as testing a GUI required some additional software to simulate a user testing widgets and onscreen objects.

*** Sprint - Week 8
With data extraction parameters finalised, a new watershedding method implemented and a more robust selection process implemented the data was now fit for hypothesis to be tested.

Through meetings with researching staff, the five key hypothesis/null-hypothesis had been formed and the grouping of the data well defined. This allowed for initial Null-Hypothesis-Significance-Testing (NHST) to be carried out. The initial results found were not representative of the data. A requirement for new hypothesis testing was now required.

The /PyTest/ library was decided upon, this was used as a unit testing framework for both the Analysis Library and the GUI software. In total around 50 tests were devised.

*** Sprint - Week 9
More graphing options were added to the GUI, these came from the boxplot functionality which the updated CT Analysing Library made available. A new type of grouping by sub-type was added. This let multiple plots be displayed for the same feature i.e. viewing a histogram of volume but split by genotype. This new feature required a rework of the dynamic plotting functionality. This refactoring took a considerable amount of time and work.

An initial solution to hypothesis testing was added in the form of Welch and Student T-Tests to first the CT Analysing Library and then to the GUI also. The GUI now had an additional tab added specifically for testing groups of data.

*** Sprint - Week 10

A previous issue was highlighted in meetings of the data being ill-suited to normally accepted statistical testing and NHST in general. Through research a new method was devised that was able to use Bayesian statistics to carry out hypothesis testing cite:Kruschke2012. This new method of hypothesis testing was added into the CT Analysing Library, requiring a lot of work in data transforming and preparation in order for correct, robust and repeatable model fitting to be carried out.
Previous research questions were repeated, using this method. This time new and significant findings were uncovered.

*** Sprint - Week 11
A feature freeze was enforced during the 11th week of the project. This was to ensure time was allocated for completing work, finalising unit tests and documentation. The > 50 unit tests were finalised, completed and ensured to be passing. Documentation was completed and exported.


\clearpage
bibliography:library.bib
bibliographystyle:IEEEannotU

* Footnotes
