#+TITLE: *Modelling the effects of domestication in Wheat through novel computer vision techniques*
#+OPTIONS: title:nil toc:nil H:4 author:nil date:nil TeX:t LaTeX:t  ^:nil
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+INCLUDE: "./preamble.org"


* Introduction, Analysis and Objectives

This project aims to answer a biological research question through the use of computer science, whilst also creating a software suite which will enable further studies to be carried out with ease.

Primarily the focus has been on the data science elements of my degree, creating, cleaning and discerning meaning in it.

Using a population of genetically diverse wheat, several hypothesis and questions are explored in the hopes of contributing to the scientific understanding of domestication. A mixture of image analysis through three-dimensional micro-computed tomography and computational analysis are used to provide these much needed solutions.

Additionally, as this is very much multi-disciplinary research, specific terms and definitions have been outlined in the /glossary/ (table:[[tab:glossary]]).

** Background

   Western society and agriculture has been dominated by the ability to create successful crops for the past 10,000 years cite:Ozkan2002. Of these crops wheat is considered to be one of the most vital and is estimated to contribute to 20% of the total calories and proteins consumed worldwide, and accounts for roughly 53% of total harvested area (in China and Central Asia) cite:Shiferaw2013.

During domestication, the main traits selected for breeding were most likely plant height and yield. This meant that important non-expressed traits such as disease resistance and drought tolerance were often neglected and lost overtime.

Whilst the choices made for selective breeding were successful, effects are now being felt as it is estimated that as much as a 5% dip is observed yearly on wheat production cite:Shiferaw2013. This decrease in efficiency is attributed to climate change bringing in more hostile conditions, which these elite and  domesticated genotypes are unprepared for.

Furthermore, with increasing populations and less arable land there is an even greater pressure for the optimisation of grain and spike characteristics. With studies showing that spikelet count can be controlled by specific and sometimes recessive genes cite:Finnegan2018, which could drastically enhance overall yield, and a general public distrust towards genetically modification cite:Aleksejeva2014,Twardowski2015,Lynas the reliance on breeding programs for optimisation is further stressed.

Modern breeding programs have had some success in selecting primitive undomesticated genotypes and using them to breed back in useful alleles which would have been lost during domestication cite:Charmet2011.

As such, there are questions still left open about how best to make selections for crop breeding. There is also a lack of formalised modelling of information which could be of use to these areas of research.

** Biological Question and Materials

The driving question for this research asks "Can \micro-CT data be used to model domestication in wheat?". Using an already grown and harvested range of genetically diverse wheat this project has generated a collection of 3D images, processed these images into raw phenotypic data and produced biologically significant information.

The genotypes used in this study are listed here, denoted by "/X/ N" where /X/ indicates the ploidy. 2N - Diploid; 4N - Tetraploid; 6N - Hexiploid.

#+BEGIN_EXPORT latex
\begin{multicols}{3}

  \begin{itemize}
  \item Wild Monococcum (2N)
  \item Domesticated Monococcum (2N)
  \item Tauschii (2N)
  \end{itemize}

  \columnbreak

  \begin{itemize}
  \item Durum (4N)
  \item Dicoccoides (4N)
  \item Dicoccum (4N)
  \item Ispahanicum (4N)
  \item Timopheevii (4N)
  \end{itemize}

  \columnbreak

  \begin{itemize}
  \item Spelta(6N)
  \item Aestivum (6N)
  \item Compactum (6N)
  \end{itemize}

\end{multicols}
#+END_EXPORT
Full species names are found in table:[[tab:wheat]].

*** Why use \micro-CT image analysis?
In the past, science has been greatly limited by the amount of data which could be processed in an experiment. In the last few decades the inclusion of computer science has reduced this bottleneck. Now, the challenge for many fields of research is producing more data and this is often cited as the major bottleneck in creating robust studies cite:Furbank2011.

Many experiments aim to meet the demand for data by using high-throughput automated imaging systems cite:Naumann2007,Prasanna2013,Humplik2015. These systems have, in the last decade, become a standard and accepted tool for data generation. However, they will only produce 2-dimensional data on a per-plant basis. Image processing research has had success in modifying these automated systems in order to produce a pseudo 3-dimensional structure using stero-imaging cite:Roussel2016. Even so, these techniques require destructive harvesting of materials and do not provide information of internal structure.

For decades medical research has found success with X-Ray imaging technology cite:Wang2008. From this, plant science has been able to benefit from the wealth of prior knowledge and more and more studies are being augmented with the use of X-Ray/\micro-CT imaging cite:Jhala2015,Tracy2012,Metzner2015,Hughes2017,Staedler2013.

In this study, \micro-CT has enabled the study of individual seeds of wheat, which is the product that plant breeders, commercial growers and farmers are truly interested in. Other imaging techniques could not provide as much detail, or in such a high throughput or quality.

*** Extracted Data

 These samples come from over 70 plants and provided in excess of 2000 seeds for analysis which data was created based on. The traits recorded are labelled in figure:[[fig:seeds]] and are as follows:

 #+BEGIN_EXPORT latex
   \begin{multicols}{2}

     \begin{itemize}
     \item Length
     \item Width
     \item Depth

     \end{itemize}

     \columnbreak

     \begin{itemize}
     \item Volume
     \item Surface Area
     \item Crease Depth / Volume
     \end{itemize}
   \end{multicols}
 #+END_EXPORT


 #+CAPTION: Wheat grain labelled (/left/), wheat grain cut in half (/right/), adapted from Hughes et al. cite:Hughes2017
 #+ATTR_LATEX: :width 17cm
 #+NAME: fig:seeds
 [[./images/seeds.png]]


** Significance to Current Research
The biological interest in this area has been expressed in several areas of research cite:Leigh2013, it is proposed that the key to unlocking diversity in the wheat genus lies in these ancestor, undomesticated species cite:Cockram2007.

This research has the potential to be useful in several areas including: crop breeding; disease resistance; environmental stress. Each of these areas depend on making informed decisions in order to direct experiments. By producing information at an individual seed level, this study has been able to provide data that can offer suggestions of plant potential and behaviour.

Often, the most sought after traits are centred around thousand-grain-weight (TGW) as well as standard deviation of seed shapes. During harvesting, filters are used to only allow ideal shaped seeds through. This means that, potentially, despite a breed of wheat providing a high average volume of seed in reality much of it may go to waste if the shapes are not uniform. This research aims to alleviate this problem and provides low level information which is sorely required.

The individual images in figure:[[fig:phylo]] show, at a glance, the diversity and also the difference in the wild and cultivated (domesticated)
species. This work allows for these differences to be quantified and evaluated into useful metrics for answering research based questions.

By better understanding the morphometric deviations in wheat species, more informed choices can be made when it comes to breeding wheat for the future and to fulfil ever-changing requirements.

\clearpage

#+CAPTION: Phylogeny of wheat genotypes (Provided by Dr. Hugo Oliveira)
#+ATTR_LATEX: :width 17cm
#+NAME: fig:phylo
[[file:./images/philotree.png]]

** Aims and Objectives

The overarching aim of this project has been to create several pieces of software which aid in answering the biologically significant questions outlined. As well as to prove/disprove the hypothesis stated below.

The software created is robust in order to duplicate results and is flexible as to allow for further studies to be carried out and to use the same method.

Novel additions have been made to existing image analysis libraries in order to make them more flexible for this project. Figure:[[fig:spikes]] illustrates the range of diversity

Furthermore, the library written allows for easy data organisation and automation of otherwise difficult tasks such as concatenating data from multiple sources and graphing of information. Full documentation and integrated testing allows for a suite of tools which can be built upon in future and reduce the amount of effort required for similar studies to be carried out and analysed.

These aims have a focus on the phenotypic attributes generated from customised image analysis software cite:Hughes2017 and can be seen in figure:[[fig:seeds]].


** Hypothesis
To provide a full spectrum of analysis the null-hypothesis of this work is presented as investigating if there are morphometric differences in the seeds of several wheat varieties outlined in figure:[[fig:phylo]].

The comparison pairs are as follows:

1. Monococcum Wild and Monococcum Domesticate
2. Dicoccoides and Dicoccum
3. Spelta and Aestivum
4. Dicoccum and Durum
5. Monococcum Wild and Dicoccoides


#+CAPTION: Scans of wheat, showing diversity in Population, Compactum (6N) left, Durum right (4N)
#+ATTR_LATEX: :width 10cm
#+NAME: fig:spikes
[[./images/spikes.png]]

** Challenges Overview

The challenges which this project tackles come in two flavours: Computational and Biological. As such keen awareness of these is needed to appreciate the novelty of this work.

*** Biological Challenges
Previous studies have been able to demonstrate that variation in wheat grain morphology can be partially explained, in 2010 Gegas et al. demonstrated this through a 99.4% 2 component PCA cite:Gegas2010. However there is much left to do in terms of formal classifications and descriptions of these differences. This project deals with this problem through computational analysis.

Two effects run parallel in this study which requires acute biological knowledge of in order to make correct decisions:

1. The effects of ploidy in wheat.
2. The effects of domestication in wheat.

Hypothesis are required to take into account, both of these effects so as not to misidentify results.

*** Computational Challenges
Using \micro-CT data in plant sciences is becoming more and more common cite:Tracy2017,Jhala2015,Hughes2017,Metzner2015 and whilst a lot of studies focus on the traits of grains specifically no formal model has been created, no accepted data format. This is a data engineering problem and the methods described in this project address this.

Further to data organisation, proposals are made for the statistical analysis which should be used. This allows for studies to become more robust and repeatable, thus strengthening the studies overall.

The biological material used in this research is much more diverse a population than has been previously studied with \micro-CT image analysis, this requires current computer vision methods to be adapted in order to be accurate.

** Deliverables

This project provides three final deliveribles:

1. A flexible software suite written in /Python/ that provides a standardised method for analysing and interpreting \micro-CT data output.
2. A Graphical User Interface (GUI) which offers a point and click method for data gathering, graphing and manipulating \micro-CT data, using the library from deliverable 1 as a backend.
3. Answers to the proposed questions (hypothesis), the /Results/ and /Discussion/ sections of this report provides this.

* Software Design, Implementation and Testing
This chapter outlines choices and methodologies employed in the software engineering aspect of this project, as well as highlighting the key functional requirements and implementation decisions.

** Functional Requirements
Requirements for this project are split between software requirements for both the CT Analysing Library and the CT GUI Application and the research requirements (i.e. the answers to the proposed hypothesis). Here the requirements for the software are discussed:
*** Requirements for CT Analysing Library

These are the functional requirements for the Python library produced:

#+BEGIN_EXPORT latex
\begin{multicols}{2}

  \begin{itemize}
  \item Provide an OOP means to deal with data
  \item Make gathering of data simplified
  \item Handle Saving of data in a useable format
  \item Easily enable data transformations
  \item Perform hypothesis testing
  \end{itemize}

  \columnbreak

  \begin{itemize}
  \item Process rejoining of split scans
  \item Handle Removing of erroneous data
  \item Enable matching data to external information
  \item Auto plot data (boxplots, histograms etc.)
  \item Allow easy filtering of data
  \end{itemize}

  \columnbreak

\end{multicols}
#+END_EXPORT

*** Requirements for CT GUI Application

#+BEGIN_EXPORT latex
\begin{multicols}{2}

  \begin{itemize}
  \item Provide a intuitive user interface for working with CT data
  \item Allow a interaction with data without the need for programming
  \item Implement the Matplotlib plotting utility
  \item Easily join experiment data with CT data
  \item Use an MVC model
  \end{itemize}

  \columnbreak

  \begin{itemize}
  \item Implement the CT Analysis Library
  \item Display data visually
  \item Dynamically create graphs
  \item Provide hypothesis testing
  \end{itemize}

  \columnbreak

\end{multicols}
#+END_EXPORT

** Software Development Methodology
This project made use of formal design methods and strict organisation whilst being flexible to change. Overall the design took a hybridised form in order to best suit the scientific environment which this domain specific software is built for.

Data analysis drove the direction of the project, as a result an agile methodology was adopted.
Weekly sprints were implemented as a list of "todo's", these were written on a Monday morning based off of the previous week's list.

Critical self-evaluation was performed by means of a "one-man SCRUM" meeting, this is a technique which requires self-discipline in order to accurately find faults and areas for improvement cite:Andrews.

Further to this, regular meetings with research staff, at the National Plant Phenomics Centre,  allowed for a developer-client relationship which SCRUM defines as being key. During these meetings details of the research was discussed and ideas given as to how future experiments could proceed. This allowed for critical decisions to be made as to software design and overall structure.
** Language Choices
Both the CT Analysing Library and the CT Analysing GUI are implemented using the Python programming language, it has been developed and tested in versions 3.5 and 3.6 (Python 2 is not supported at all by this project).

In scientific programming three of the most commonly used languages are Python, R and MATLAB cite:Ozgur2016.

These three languages are able to provide all the features which this project requires. However Python was chosen for several reasons.

MATLAB could not be used as a potential language due to it being pay to use software, as this project aims to be accessible, the cost of software would greatly reduce the scope of access.

R is a valid candidate, it provides all of the statistical capabilities required by the project, it also provides packages for creating GUI based applications, it is fast and it is widely used in scientific computing and data science.

The main deciding factor is Python's wealth of resources, adoption rate and the developer of this project being vastly more experienced with Python's ecosystem than R's.

** Designing Process
Through meetings and emails, the agile principles of communication over comprehensive documentation was used. Where conversations were decidedly much more beneficial than complex planing prior to developing a product.

Graphical elements, such as the graphing functionality of the CT Analysing Library and the CT GUI Application were sketched using wire-frames whilst in meetings where the potential users (clients) could provide their ideas.

In figure:[[fig:gui1]] an example of the wire-frames created during meetings is show (A), next to it is displayed the final look of the loading window (B).
 #+CAPTION: Wire-frame of the GUI loading data window
 #+ATTR_LATEX: :width 16cm
 #+NAME: fig:gui1
 [[./images/wireframe1.png]]

Similarly, figure:[[fig:gui2]] provides the initial wire-frame (A) of how the analysis window could have looked and what kind of GUI elements would be required, again, next to it is the final analysis window (B)
 #+CAPTION: Wire-frame of the GUI analysis window
 #+ATTR_LATEX: :width 16cm
 #+NAME: fig:gui2
 [[./images/wireframe2.png]]
** Documentation
Whilst an agile approach was used, some documentation was created for use with the CT Analysing Library.

The provided CT Analysing Library comes with "human-readable" format. Where most documentation generators (Doxygen, Pydocs, Javadocs etc.) implement very well structured and comprehensive documentation, the output is generally not very friendly and easy to read. Particularly for non-career-programmers. A core feature of these provided software implementations are that they are well suited for a biologist, researcher or statistician to use.

This documentation generator was purpose created, implemented in LISP and provided in listing:[[lst:docgen]].

Beyond this, inline commenting is provided for supplied software. Keeping in line with the agile development ethos the software is self-documented and self-evident. A brief example of this is shown in listing:[[lst:docexample]]

Documentation for the CT GUI application is provided as a visual user guide, and provides sample data for the user to test with.
** Software Library Choices
The software libraries used for this project focus around data manipulation, where possible core libraries of the Python language were used and only well supported, established and documented libraries were chosen. Software support is a major requirement for reproducible results.

All software packages used in the Analysing Library are required by the CT Analysing GUI as the Library is a dependency of it. The GUI has a single separate requirement /PyQT5/.
Table:[[tab:software]] contains a full listing of all software used and required by this project.
*** CT Analysing Library
**** Numpy
The Numpy library is one of the most commonly used additions to the Python ecosystem, it is fundamental to many data science projects. Here it is used to handle data lists, arrays and structures. There is no viable alternative to this package and it is required by Scipy and Statsmodels.
**** Matplotlib/Seaborn
Matplotlib acts as the plotting backend for the project. The Seaborn package acts as a porcelain for matplotlib and makes graph creation and decoration much easier.
**** Scipy
Data transforms such as Box Cox and PCA are dependant on the functions of the Scipy library. Alternatives are available, however this is the most well established and often used library for these functions.
**** Pandas
Pandas is used to read the CSV files which the raw data is stored in. This library converts and stores data in dataframes which are used throughout this project to manipulate data.
**** Xlrd
This extension library is required in order to read Microsoft encoded files. Extra experiment information can be provided with the "xlsx" extension.
**** Statsmodels
Bayesian hypothesis testing is provided through this library.
**** PyQT5
There were many options for creating a user interface in Python, the language provides its own core library via the /TKinter/ module. However PyQT is a port of the QT framework, one of the most widely used libraries for GUIs in software development. It is cross platform, robust and has excellent documentation and user-guides.

** Implementation
Strict software engineering principles were applied during creation of this project. The use of standards, design patterns and code-linters have been used throughout to minimise the possibility of errors and to create wholly extendable software. These devices enable understandable and self-documented code allowing future users to quickly start using the provided packages.

*** Design Patterns
Each of the packages of this projects follow a design pattern to make them extendable for future additions, as and when they are required by future studies and experiments.
**** CT Analysing Library Design Pattern
The CT Analysing Library uses a Singleton style design pattern. A single data object is created from a /CTData/ class.

A very functional paradigm is used by this library. By applying mapping and filter style functions data elements can be passed to the supporting modules: /data_transforms.py/; /graphing.py/, /statistical_tests.py/. These modules enable scientific functions to be applied to the /CTData/ object. A UML style class diagram is shown in figure:[[fig:ctdata]], here the interactions of the classes can be seen, as well as their internal functions.
#+BEGIN_SRC  plantuml :results file :file ./images/ctdata.png
skinparam monochrome true
skinparam rectangleFontSize 30
skinparam classFontSize 30
skinparam classAttributeFontSize 30
  skinparam linetype ortho
  left to right direction
  class data_transforms  {

  +box_cox_data()
  +standardise_data()
  +perform_pca()
  +pca_to_table()

  }

  class graphing {

  +plot_difference_o_means()
  +plot_forest_plot()
  +plot_boxplot()
  +plot_qqplot()
  +plot_histogram()
  +plot_pca()
  +check_var_args()

  }


  class CTData {
  + __init__()
  + get_data()
  + gather_data()
  + create_dimensions_ratio()
  + make_dataframe()
  + clean_data()
  + get_files()
  + fix_colnames()
  + join_spikes_by_rachis()
  + remove_percentile()
  + aggregate_spike_averages()
  + find_troublesome_spikes()
  + gather_data()
  }



  class statistical_tests{

  +baysian_hypothesis_test()
  +check_normality()
  +perform_t_test()

  }

  data_transforms <-- CTData
  CTData --> statistical_tests
  CTData --> graphing
#+END_SRC
#+NAME: fig:ctdata
#+CAPTION: CT Analysing Library UML
#+ATTR_LATEX: :width 12cm
#+RESULTS:
[[file:./images/ctdata.png]]

**** CT GUI Application Design Pattern
The Model-View-Controller (MVC) design pattern is one of the most commonly structures for creating user interfaces. It allows for the user's view/interface code to be separated from the model, the code which changes the data. The model and the view communicate and update each other via the controller element of the design.

The QT framework provides "connectors" which act as triggers/activations for functions, these are set off by the user providing either keyboard or mouse based input.

#+BEGIN_SRC  plantuml :results file :file ./images/ctgui.png
skinparam monochrome true
skinparam rectangleFontSize 30
skinparam classFontSize 30
skinparam classAttributeFontSize 30
skinparam linetype ortho

left to right direction
rectangle View{


    class UiMainWindow{
    +setupUi()
    +retranslateUi()
    }

    class MatplotlibView{
    +__init__()
    +setup()
    }
    }


  rectangle Controller{

    class AnalysisWindow{
    +__init__()
    +set_graph_type()
    +refresh()
    +update_view()
    +setup_figure_canvas()
    +find_clicked_button()
    +get_group_by()
    +make_canvas()
    }

    class AppWindow{

    +__init__()
    +setup_menu_functions()
    +setup_default_states()
    +view_data()
    +get_data()
    +set_data()
    +update_analysis()

    }


    class FindFilesWindow{
    +__init__()
    +connect_view_functions()
    +set_default_states()
    +find_files()
    +save_file_dialog()
    +search_for_files()
    +set_files_list()
    }



    class PreProcessWindow{
    +__init__()
    +connect_view_functions()
    +select_experimental_info()
    +load_experimental_data()
    +clean_data()
    }



    class StatsTestWindow{

    +__init__()
    +connect_view_functions()
    +set_test_type()
    +set_group_by()
    +update_view()
    +setup_figure_canvas()
    +get_group_by()
    +slice_data()
    +make_canvas_plot()

    }

    }

    rectangle Model {

    class CTGUIData{
    +__init__()
    +download_data()
    }

    class WidgetList{
    +__init__()
    +update()
    +get_items()
    }

    class MyMplCanvas {
    +__init__()
    +facet_hist()
    +compute_initial_figure()
    }

    class MyStaticMplCanvas {
    +compute_initial_figure()
    }

    class PandasModel{
    +__init__()
    +setup()
    }

    }

  Model -->  Controller
  View --> Controller

  Controller --> Model
  Controller --> View

#+END_SRC
#+NAME: fig:ctgui
#+CAPTION: CT Analysing GUI UML
#+ATTR_LATEX: :width 15.2cm
#+RESULTS:
[[file:./images/ctgui.png]]

*** Standards
The main standard adhered to for software provided by this project is the PEP8 style guide cite:VanRossum. The principle behind this coding style, as stated by Guido van Rossum, is "Code is read much more often than it is written". This makes this styling guide perfect for the chosen agile methodology of self-evident documentation in the software.

In addition to PEP8, a Python code linter Flake8 has been used to prevent "code smells", bad formatting, incorrect white space usage etc.

** Version control
This project has used Git version 2.7.4 throughout. The structure of the project has been as submodules of a larger project.

By using submodules the CT Grain Analysing Library could be kept in sync with the GUI aspect of the project.

Additionally, /setup.py/ has been used to provide installation of the library, the code for this can be seen in listing:[[lst:setuppy]]. Using /setup.py/ provides a quick and easy way for any user to install the software, along with any dependencies.

**** Issue Tracking
Issues were tracked during the project, both in personal notes and in the Git interface as illustrated in figure:[[fig:github]]
#+CAPTION: Github Issue Tracking
#+NAME: fig:github
#+ATTR_LATEX: :width 12cm
[[./images/github.png]]

** Library Versioning
The development of this project was performed using the Python /virtualenv/. This is a virtual environment package which Python offers, it allows for an isolated working copy of the project.

By developing in this manner, libraries were ensured to be using the correct versions required by the software.
 \clearpage
** Testing
Testing was performed both in acceptance testing by using user feedback, the functional requirements and the ability to use the software to answer the hypothesis of the research elements of this project. Further to this, unit testing was performed to allow for automated testing as well as test-driven-development of features.
*** Feedback Forms
Feedback and constructive suggestions were made by researchers at the National Plant Phenomics Centre, these were submitted via the Google forms service.

These provided a method of acceptance testing by those who would be using the software to help with investigating data. Page 1 of the given form is shown in figure:[[fig:feedback]]. This form was completed by 3 researchers at the National Plant Phenomics Centre, feedback was very positive overall.
#+NAME: fig:feedback
#+CAPTION: CT Feedback form
#+ATTR_LATEX: :width 7.3cm
[[./images/feedbackform.png]]

\clearpage

*** Unit Testing CT Analysing Library
The unit tests for the CT Analysing Library were straightforward, using the /PyTest/ framework and a subset of data from a data set, these tests assert that features are implemented correctly and that the correct results are given.


#+ATTR_LATEX: :environment tabularx :width \textwidth :align |l|X|
#+NAME: tab:unittest
#+CAPTION: Output of /pytest/ Unit Tests and results for CT Analysing Library
|---------------------------+----------------------------------------------------|
| *Result*                  | *Test*                                             |
|---------------------------+----------------------------------------------------|
| \color{ForestGreen}Passed | CTData.py::test_aggregate_spike_averages           |
|---------------------------+----------------------------------------------------|
| \color{ForestGreen}Passed | CTData.py::test_clean_data_maximum_removed         |
|---------------------------+----------------------------------------------------|
| \color{ForestGreen}Passed | CTData.py::test_clean_data_minimum_removed         |
|---------------------------+----------------------------------------------------|
| \color{ForestGreen}Passed | CTData.py::test_load_additional_data               |
|---------------------------+----------------------------------------------------|
| \color{ForestGreen}Passed | CTData.py::test_load_additional_data_no_data       |
|---------------------------+----------------------------------------------------|
| \color{ForestGreen}Passed | CTData.py::test_load_data                          |
|---------------------------+----------------------------------------------------|
| \color{ForestGreen}Passed | CTData.py::test_NoDataFoundException               |
|---------------------------+----------------------------------------------------|
| \color{ForestGreen}Passed | Data_transforms.py::test_box_cox_data              |
|---------------------------+----------------------------------------------------|
| \color{ForestGreen}Passed | Data_transforms.py::test_pca_to_table              |
|---------------------------+----------------------------------------------------|
| \color{ForestGreen}Passed | Data_transforms.py::test_perform_pca               |
|---------------------------+----------------------------------------------------|
| \color{ForestGreen}Passed | Data_transforms.py::test_standardise_data          |
|---------------------------+----------------------------------------------------|
| \color{ForestGreen}Passed | Graphing.py::test_plot_boxplot_as_dataframe        |
|---------------------------+----------------------------------------------------|
| \color{ForestGreen}Passed | Graphing.py::test_plot_boxplot_as_object           |
|---------------------------+----------------------------------------------------|
| \color{ForestGreen}Passed | Graphing.py::test_plot_difference_of_means         |
|---------------------------+----------------------------------------------------|
| \color{ForestGreen}Passed | Graphing.py::test_plot_histogram_as_dataframe      |
|---------------------------+----------------------------------------------------|
| \color{ForestGreen}Passed | Graphing.py::test_plot_histogram_as_object         |
|---------------------------+----------------------------------------------------|
| \color{ForestGreen}Passed | Graphing.py::test_plot_pca                         |
|---------------------------+----------------------------------------------------|
| \color{ForestGreen}Passed | Graphing.py::test_plot_qqplot                      |
|---------------------------+----------------------------------------------------|
| \color{ForestGreen}Passed | Statistical_tests.py::test_baysian_hypothesis_test |
|---------------------------+----------------------------------------------------|
| \color{ForestGreen}Passed | Statistical_tests.py::test_t_test                  |
|---------------------------+----------------------------------------------------|
| \color{ForestGreen}Passed | Statistical_tests.py::test_test_normality          |
|---------------------------+----------------------------------------------------|


\clearpage

*** Unit Testing CT GUI Application
The unit testing used for the CT GUI Application was more sophisticated than that of the Library. This testing required visual confirmation that figures and graphs generated were displayed correctly and that they showed what the user would expect, given the data.

To do this a /PyTest/ plugin was used, /QtBot/ which provides simulated user input. This allows for the GUI to be thoroughly tested, automatically.

In table:[[tab:unittest2]] the results of the automated testing is given along side an image of several of the tests, tests of the same graphs but with different parameters were also generated and manually verified and provided as supplemental data.

#+ATTR_LATEX: :environment longtable :width \textwidth :align |l|p{4.4cm}|C|
#+NAME: tab:unittest2
#+CAPTION: Output of /pytest/ Unit Tests and results for CT GUI Application
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| *Result*                  | *Test*                                                 | *Image*                                                      |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | analysis.py:: box_groupby_1_rb_1                       | [[./images/Screenshots/analysis_window_box_groupby_1_rb_1.png]]  |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | analysis.py:: box_groupby_2_rb_2                       | [[./images/Screenshots/analysis_window_box_groupby_2_rb_2.png]]  |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | analysis.py:: box_rb_1                                 | [[./images/Screenshots/analysis_window_box_rb_1.png]]            |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | analysis.py:: hist_groupby_1_rb_1                      | [[./images/Screenshots/analysis_window_hist_groupby_1_rb_1.png]] |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | analysis.py:: hist_rb_1                                | [[./images/Screenshots/analysis_window_hist_rb_1.png]]           |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | hypothesis_tests.py:: bayesg1_att_1                    | [[./images/Screenshots/hypothesis_bayestest_g1_att_1.png]]       |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | hypothesis_tests.py:: tg1_att_1                        | [[./images/Screenshots/hypothesis_ttest_g1_att_1.png]]           |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | analysis.py:: box_rb_2                                 | N/A                                                          |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | analysis.py:: hist_groupby_1_rb_2                      | N/A                                                          |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | analysis.py:: hist_rb_2                                | N/A                                                          |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | analysis.py:: loads                                    | N/A                                                          |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | hypothesis_tests.py:: bayesg1_att_2                    | N/A                                                          |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | hypothesis_tests.py:: bayesg2_att_1                    | N/A                                                          |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | hypothesis_tests.py:: bayesg2_att_2                    | N/A                                                          |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | hypothesis_tests.py:: tg1_att_2                        | N/A                                                          |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | hypothesis_tests.py:: tg2_att_1                        | N/A                                                          |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | hypothesis_tests.py:: tg2_att_2                        | N/A                                                          |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | hypothesis_tests.py:: loads                            | N/A                                                          |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | GUI.py:: startup                                       | N/A                                                          |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | load_data.py:: load_data_with_rachis                   | N/A                                                          |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | load_data.py:: load_data_without_rachis                | N/A                                                          |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | preprocessing.py:: clean_data_remove_large             | N/A                                                          |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | preprocessing.py:: clean_data_remove_none              | N/A                                                          |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | preprocessing.py:: clean_data_remove_small             | N/A                                                          |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | preprocessing.py:: clean_data_remove _small_and_large  | N/A                                                          |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | preprocessing.py:: load_additional_data                | N/A                                                          |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|
| \color{ForestGreen}Passed | preprocessing.py:: load_additional_data _expected_fail | N/A                                                          |
|---------------------------+--------------------------------------------------------+--------------------------------------------------------------|

* Methods and Solutions
** Data Pipeline

#+BEGIN_SRC  plantuml  :file ./images/matlab.png
skinparam monochrome true
skinparam ActivityFontSize 30
skinparam ArrowFontSize 30
skinparam PartitionFontSize 30
skinparam ActivityDiamondFontSize 30

(*) -> "Scan wheat Spikes"

partition MATLAB {
--> "Load Raw Scan Data"
-> "Convert to 2D Image Stacks"
--> "Calculate Threshold Percentile"
-left-> "Segment and Mask via Threshold"
--> "Calculate 3D Quasi-Euclidean Distance"
If "Seeds Joined" then
--> [Yes] "Separate by Watershed"
--> "Detect Objects"
else
-> [No] "Detect Objects"
Endif



-> "Record Measurements"
--> [For each seed] "Record Measurements"

--> "Output Data to CSV format"
}
->(*)

#+END_SRC
#+NAME: fig:matlab
#+CAPTION: Image Processing Pipeline
#+ATTR_LATEX: :width 13cm
#+RESULTS:
[[file:./images/matlab.png]]

** Image Analysis Methods
***  [#B] New Watershed Algorithm

In order to solve the problem of misidentified and joint seeds, from the primitive collection,
a  /quasi-euclidean/ distance transform was implemented into the analysis pipeline (figure:). This provided much better results than the previous
/chessboard/ transform which had been successful on more uniform data in previous studies cite:Hughes2017.

**** Quasi-Euclidean algorithm

This algorithm measures the total euclidean distance along a set of horizontal, vertical and diagonal
line segments cite:Pfaltz1966.

#+NAME: eqn:qe
\begin{equation}
\left | x_1 - x_2 \right | + (\sqrt{2}-1), \left | x_1 - x_2 \right | >\left | y_1 - y_2 \right | (\sqrt{2}-1) \left | x_1 - x_2 \right | ,\textup{otherwise}
\end{equation}



In order to apply this to a 3D space Kleinberg's method is used  cite:Kleinberg1997. This allows for nearest neighbour pixels to be sorted by $k$-dimensional trees
and enabling fast distance transforms via Rosenfeld and Pfaltz's /quasi-euclidean/ method stated in equation:[[eqn:qe]].
**** Effect of Enhanced Watershed algorithm
#+BEGIN_CENTER
#+CAPTION: /A/ showing the chessboard method, /B/ improved quasi-euclidean method
#+ATTR_LATEX: :width 10cm
#+NAME: fig:qe
[[./images/chess_quasi.png]]
#+END_CENTER

*** Extracted Grains

 #+CAPTION: Individual Wheat grains, rendered in 3D
 #+ATTR_LATEX: :width 13cm
 #+NAME: fig:ctgrains
 [[./images/ctgrains.png]]

** CT Analysing Library Methods
** CT GUI Application Methods
** Data Analysis Methods

#+BEGIN_SRC  plantuml :results file :file ./images/pipeline.png
skinparam monochrome true
skinparam ActivityFontSize 30
skinparam ArrowFontSize 30
skinparam PartitionFontSize 30
skinparam ActivityDiamondFontSize 30

(*) -> "MATLAB Output"
partition Python {
-right-> "Load and Convert data to CTData Objects"

If "Check for \nfalse positives?" then
-> [Yes]"Estimate Error"
--> "Remove Estimated Outliers"
--> "Load in external experiment information"
else
-> [No] "Load in external experiment information"
Endif

If "Spike Joining?" then
-left-> [Yes] "Process logical spike joining"
--> "Aggregate Data columns"
else
--> [No] "Aggregate Data columns"
Endif

If "Normally\nDistributed?\n" then
--> [Yes] "Perform Parametric Tests"

--> "Output Results"
else
--> [No] "Perform Non-Parametric Tests"
Endif

--> "Output Results"
}
#+END_SRC
#+NAME: fig:pipeline
#+CAPTION: How data is integrated with the CT Analysing Library
#+ATTR_LATEX: :width 13cm
#+RESULTS:
[[file:./images/pipeline.png]]

* Results
* Discussion
** Similar Research
** Alternate Solutions
* Critical Evaluation
** Organisational Methods
** Relevance to Degree
** Time Management
** Collaborative Work
** Other Issues
* Appendix
** /Software Packages Used/

*** Libraries
#+ATTR_LATEX: :environment tabularx :width \textwidth :align |X|X|X|
#+NAME: tab:software
#+CAPTION: Software libraries used
|---------------------------------+-------+------------|
| Seaborn                         | Scipy | Sklearn    |
|---------------------------------+-------+------------|
| MATLAB Image Processing Toolbox | Numpy | Matplotlib |
|---------------------------------+-------+------------|
| Statsmodels                     | Pymc3 | Xlrd       |
|---------------------------------+-------+------------|
| PyQt5                           | gcc   | Pip        |
|---------------------------------+-------+------------|

*** Tools
#+ATTR_LATEX: :environment tabularx :width \textwidth :align |X|X|X|
#+NAME: tab:softwareused
#+CAPTION: Software tools used
|--------+-----------------------+----------|
| MATLAB | Python Debugger (PDB) | IPython  |
|--------+-----------------------+----------|
| Emacs  | git                   | org-mode |
|--------+-----------------------+----------|
| Tomviz | ImageJ                | PlantUML |
|--------+-----------------------+----------|

\clearpage
** /Glossary/
#+ATTR_LATEX: :environment tabularx :width \textwidth :align |l|X|
#+NAME: tab:glossary
#+CAPTION: Dictionary for Terms and acronyms
|--------------+------------------------------------------------------------------------------|
| *Term*       | *Definition*                                                                 |
|--------------+------------------------------------------------------------------------------|
| \micro-CT    | Micro Computed Tomography                                                    |
|--------------+------------------------------------------------------------------------------|
| Genotype     | A genetically distinct individual or group                                   |
|--------------+------------------------------------------------------------------------------|
| Phenotype    | A physical/measurable trait                                                  |
|--------------+------------------------------------------------------------------------------|
| Alleles      | A variant of a gene                                                          |
|--------------+------------------------------------------------------------------------------|
| Genus        | Classification ranking, below the /family/ grouping                          |
|--------------+------------------------------------------------------------------------------|
| Genome       | The complete genetic make up of an organism, which defines its individuality |
|--------------+------------------------------------------------------------------------------|
| Morphometric | The shape and form of an organism                                            |
|--------------+------------------------------------------------------------------------------|
| GUI          | Graphical User Interface                                                     |
|--------------+------------------------------------------------------------------------------|
| PCA          | Principal Component Analysis                                                 |
|--------------+------------------------------------------------------------------------------|
| Spike        | A singular stalk of wheat                                                    |
|--------------+------------------------------------------------------------------------------|
| Spikelet     | A group of seeds all forming from the same node in a spike                   |
|--------------+------------------------------------------------------------------------------|
| MVC          | Model View Controller - A design pattern for GUIs                            |
|--------------+------------------------------------------------------------------------------|
| OOP          | Object Orientated Programming                                                |
|--------------+------------------------------------------------------------------------------|

\clearpage
** Wheat Varieties
#+ATTR_LATEX: :environment tabularx :width \textwidth :align |X|X|
#+NAME: tab:wheat
#+CAPTION: Dictionary for Wheat names used
|-----------------+-----------------------|
| *Used name*     | *Species name*        |
|-----------------+-----------------------|
| Monococcum      | /Triticum monococcum/ |
|-----------------+-----------------------|
| Monococcum Wild | Triticum Boeticum     |
|-----------------+-----------------------|
| Tauschii        | Aegilops tauschii     |
|-----------------+-----------------------|
| Durum           | Triticum Durum        |
|-----------------+-----------------------|
| Dicoccoides     | Triticum Dicoccoides  |
|-----------------+-----------------------|
| Dicoccum        | Triticum Dicoccum     |
|-----------------+-----------------------|
| Ispahanicum     | Triticum Ispahanicum  |
|-----------------+-----------------------|
| Timopheevii     | Triticum Timopheevii  |
|-----------------+-----------------------|
| Spelta          | Triticum Spelta       |
|-----------------+-----------------------|
| Aestivum        | Triticum Aestivum     |
|-----------------+-----------------------|
| Compactum       | Triticum Compactum    |
|-----------------+-----------------------|

\clearpage
** /Code Segments and Examples/
*** MATLAB Watershedding

#+CAPTION: MATLAB Watershedding function
#+LABEL: lst:ws
#+BEGIN_SRC octave
  function [W] = watershedSplit3D(A)
    % Takes image stack A and splits it into stack W
    % Convert to BW
    bw = logical(A);
    % Create variable for opening and closing
    se = strel('disk', 5);
    % Minimise object missshapen-ness
    bw = imerode(bw, se);
    bw = imdilate(bw, se);
    % Fill in any left over holes
    bw = imfill(bw,4,'holes');
    % Use chessboard for distance calculation for more refined splitting
    chessboard = -bwdist(~bw, 'quasi-euclidean');
    % Modify the intensity of our bwdist to produce chessboard2
    mask = imextendedmin(chessboard, 2);
    chessboard2 = imimposemin(chessboard, mask);
    % Calculate watershed based on the modified chessboard
    Ld2 = watershed(chessboard2);
    % Take original image and add on the lines calculated for splitting
    W = A;
    W(Ld2 == 0) = 0;
  end
#+END_SRC

\clearpage
*** Custom Documentation Generator
#+CAPTION: Custom lisp code for generating easy to read documentation
#+LABEL: lst:docgen
#+BEGIN_SRC emacs-lisp
  (defun populate-org-buffer (buffer filename root)
    (goto-char (point-min))
    (let ((to-insert (concat "* " (replace-regexp-in-string root "" filename) "\n") ))
      (while (re-search-forward
	      (rx (group (or "def" "class"))
		  space
		  (group (+ (not (any "()"))))
		  (? "(" (* nonl) "):" (+ "\n") (+ space)
		     (= 3 "\"")
		     (group (+? anything))
		     (= 3 "\"")))
	      nil 'noerror)
	(setq to-insert
	      (concat
	       to-insert
	       (if (string= "class" (match-string 1))
		   "** "
		 "*** ")
	       (match-string 2)
	       "\n"
	       (and (match-string 3)
		    (concat (match-string 3) "\n")))))helm-semantic-or-imenu
      (with-current-buffer buffer
	(insert to-insert))))
  (defun org-documentation-from-dir (&optional dir)
    (interactive)
    (let* ((dir  (or dir (read-directory-name "Choose base directory: ")))
	   (files (directory-files-recursively dir "\py$"))
	   (doc-buf (get-buffer-create "org-docs")))
      (dolist (file files)
	(with-temp-buffer
	  (insert-file-contents file)
	  (populate-org-buffer doc-buf file dir)))
      (with-current-buffer doc-buf
	(org-mode))))
#+END_SRC

\clearpage
*** Self-Documenting Code Example
#+CAPTION: Example of code documentation and readability from /data_transforms.py/
#+LABEL: lst:docexample
#+BEGIN_SRC python
  def get_spike_info(self, excel_file, join_column='Folder#'):
      """
      This function should do something akin to adding additional
      information to the data frame

      @note there is some confusion in the NPPC about whether to use
      folder name or file name as the unique id when this is made into
      end-user software, a toggle should be added to allow this

      @param excel_file a file to attach and read data from
      @param join_column if the column for joining data is
      different then it should be stated
      """
      try:
	  # Grab the linking excel file
	  info = pd.read_excel(excel_file,
			       index_col='Folder#')

	  features = list(info.columns)
	  # Lambda to look up the feature in excel spreadsheet
	  def look_up(x, y): return info.loc[x['folderid']][y]

	  # Lambda form a series (data row) and apply it to dataframe
	  def gather_data(x): return pd.Series(
	      [look_up(x, y) for y in features])

	  self.df[features] = self.df.apply(gather_data, axis=1)
      except KeyError as e:
	  print('Error matching data')
	  print(e)
	  raise NoDataFoundException
      except AttributeError as e:
	  print(e)
	  raise NoDataFoundException

#+END_SRC
\clearpage
*** Setup.py
#+CAPTION: The /setup.py/ configuration for the CT Analyser Library
#+LABEL: lst:setuppy
#+BEGIN_SRC python
  from setuptools import setup
  setup(name='CT_Analysing_Library',
	version='0.2',
	description='Library used for CT grain analysis at the NPPC',
	url='https://github.com/SirSharpest/CT_Analysing_Library',
	author='Nathan Hughes',
	author_email='nathan1hughes@gmail.com',
	license='MIT',
	packages=['ct_analysing_library'],
	install_requires=['pandas',
			  'numpy',
			  'matplotlib',
			  'seaborn',
			  'scipy',
			  'sklearn',
			  'statsmodels',
			  'pymc3',
			  'xlrd'],
	zip_safe=True)
#+END_SRC

\clearpage
bibliography:library.bib
bibliographystyle:IEEEannotU
